{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmf0920/Feiii_code/blob/main/DL_lectureNotebook1_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDpQ-i6p0Tey"
      },
      "source": [
        "#Getting started with tensors in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRRuF0X8xnmf"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWncMBGN1rA6"
      },
      "source": [
        "# Data representation in neural networks\n",
        "\n",
        "Modern machine-learning systems use *tensors* as their basic data structures. These are fundamentally containers for data. You will already have used matrices, which are examples of two-dimensional tensors. The tensor's *rank* is its number of axes (like dimensions for a matrix)\n",
        "\n",
        "# Tensors\n",
        "\n",
        "*  Scalars: Tensors which contain only one number. 0-dimensional tensors\n",
        "*  Vectors (1D tensors). An array of numbers is a vector or 1D tensor, and has one axis.\n",
        "*  Matrices are 2D tensors. The two axes are referred to as *rows* and *columns*\n",
        "*  3D tensors and higher. Putting a 2D matrix in a new array gives you a 3D tensor (and so on to higher dimensions..). You will end up spending a lot of time making sure your tensors are the right shape, when debugging your deep networks... :-)\n",
        "\n",
        "* Manipulating tensors in Numpy\n",
        "* data batches\n",
        "* Examples of training data used, in each case with $N=$ `samples` examples in the set:\n",
        "   * Vector data - 2D `(samples, features)`\n",
        "   * Time-series/sequence data - 3D tensors shaped `(samples, timesteps, features)`)\n",
        "   \n",
        "   ![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRrnL1abPz6Nkugh29zNvaq-L_KqAvJAIBJQdr_dpi4Khpnrn_sww)\n",
        "   * Image data -- 4D tensors shaped `(samples, height, width, channels)`\n",
        "   \n",
        "   ![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbVhpi2ZiSb7fl0Q8YMXBhMQ7Ny-rwHG82TEpjgIm8yKCfiV0r)\n",
        "   * Video data - 5D tensors shaped `(samples, frames, height, width, channels)`. Think about the memory requirements of these tensors. How much memory would you need for a 60 second video at 256x256 sampled 30 times a second?\n",
        "---\n",
        "# Tensor operations\n",
        "\n",
        "* Element-wise operations - e.g. the activation function `relu()` when implemented as `torch.max(z,0.0)` is applied independently to each entry of the tensor considered (rather than using a `for` loop to run through each entry). Usually much more efficient than implementing loops. In a number of cases these element-wise operations are different from the typical operations on matrices. So element-wise multiplication corresponds to the Hadamard product $z = x \\odot y$ (sometimes written $x \\circ y$) where each element is multiplied together to get a new element rather than the typical matrix multiplication.\n",
        "* Broadcasting - when the shapes of tensors differs and there is no ambiguity, and the results are legal, the smaller tensor can be *broadcasted* to match the shape of the larger tensor.\n",
        "     * this has two steps:\n",
        "     1. axes are added to the smaller tensor to match the `ndim` of the larger (from the left-hand side)\n",
        "     2. the smaller tensor is repeated alongside any axes (assuming they are a multiple of the larger axes size) to match the full shape of the larger tensor.\n",
        "     E.g.\n",
        "     \n",
        "     `x = torch.rand(64,3,32,10)`\n",
        "     \n",
        "    `y = torch.rand(3, 32, 1)`\n",
        "    \n",
        "    `z = torch.max(x,y)` - z has shape (64,3,32,10) like `x`\n",
        "    \n",
        "* Tensor dot - aka *tensor product* . In torch use `z=torch.dot(x,y)`, in mathematical notation $z=x \\cdot y$. Note this is different from the elementwise multiplication `z=x*y`. The dot product of two vectors (which have to be the same size) is a scalar. The dot product of a matrix $x$ and a vector $y$ is a vector where the cofficients are the dot products between $y$ and the rows of $x$.\n",
        "* Tensor reshaping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8NSa2l1aCGh",
        "outputId": "09092d2b-9556-4b54-d026-44b2fb434ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=torch.ones(64, 3, 32, 10)#生成一个形状为(64,3,32,10)的张量，所有的元素都初始化为1\n",
        "y=torch.ones(3, 32, 1)#生成一个形状为(3,32,1)的张量，所有的元素都为1\n",
        "z = torch.max(x, y)#计算求解x和y的逐元素最大值，赋给z\n",
        "#因为y的维度小，所以需要先将y进行广播，将y扩展成与x相同的形状，所以要从左边增加一个轴（补充一个1），将1广播成64(与x相同)\n",
        "#中间两个轴与x值相同，所以重复，最后一个扩张到x的大小(10)\n",
        "#然后将x和广播后的y在每个位置上进行比较取最大值，因为都是1，所以最后z的每个元素也都是1\n",
        "#z的形状与x相同\n",
        "\n",
        "#分别打印出z和z的形状\n",
        "print(z)\n",
        "print(z.size())\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "torch.Size([64, 3, 32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us0Ekrytxnmp"
      },
      "source": [
        "Note that when an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\n",
        "\n",
        "请注意，当创建一个未初始化的矩阵时，分配的内存中的任何值都会作为初始值出现。（意思是如果创建一个新的张量但没有给它赋初值时，这个张量中的元素将会显示为那部分计算机内丛中已经存在的值，一般是一些随机的数据，是之前该内存地址上的遗留数据）\n",
        "\n",
        "An alternative is to explicitly set things to be random（另一种做法是明确将内容设计为**随机值**）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eQ31svwxnmq",
        "outputId": "7fc0007a-2f4d-4073-99d7-ed3cdaf478de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.rand(5, 3)#生成一个形状为[5,3](5行3列)的张量x，每个元素都是随机生成的浮点数，在[0,1)之间均匀分布\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0137, 0.0755, 0.4435],\n",
            "        [0.1069, 0.5266, 0.9001],\n",
            "        [0.1644, 0.6658, 0.6922],\n",
            "        [0.5541, 0.4629, 0.7950],\n",
            "        [0.9761, 0.1488, 0.1668]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3HbYvlUxnmt"
      },
      "source": [
        "Construct a matrix filled zeros and of dtype long（构建一个填充了零且数据类型为长整型的矩阵）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE59Jus1xnmu",
        "outputId": "c4d67eca-9ce6-4044-dcfe-ffe87b5b953e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.zeros(5, 3, dtype=torch.long)#5行3列元素全为0的矩阵\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-MIpgIJxnmx"
      },
      "source": [
        "Let us take a list of three numbers in python（在python中取一个包含3个数字的列表）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJSZM2Uxnmy"
      },
      "source": [
        "a = [1.0, 2.0, 1.0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY0NJdpFxnm1"
      },
      "source": [
        "we can access the first element of the list using the index 0（我们可以使用索引0来访问列表的第一个元素）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNCBoQNPxnm2",
        "outputId": "43a32908-5fdb-40c6-a6d6-670cbaa94cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0]#访问第一个元素"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq2XMYJHxnm4",
        "outputId": "948d7981-a165-4b41-a52d-f74906ef71a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[2] = 3.0#修改第3个元素并输出\n",
        "a"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsruzDGxnm7"
      },
      "source": [
        "We can create a PyTorch tensor（创建PyTorch张量）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCKbW_-qxnm8",
        "outputId": "9e86316b-1ab4-457e-e2a6-a87aa71f733d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "a = torch.ones(3)#创建一个包含3个元素的切全是1的1D张量\n",
        "a"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=torch.tensor(1)#创建一个值为1的0D张量也就是标量数字1\n",
        "b"
      ],
      "metadata": {
        "id": "_uSsdfH8HD7A",
        "outputId": "e45acf74-3d05-4281-e4f4-5f5dac6bab15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9-D-648xnm_"
      },
      "source": [
        "The first entry isn't a 1, it is a tensor with one element in it（第一个条目不是1，而是一个包含了一个元素的张量）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZbU3QAyxnnA",
        "outputId": "ec312409-262b-4dc3-a4af-7e95cf4cc9ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0]#是一个包含了一个元素1的张量"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbARxFEDxnnC"
      },
      "source": [
        "but you can easily convert it to a float（但是可以很轻松的将其转换为浮点数）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFoVVplYxnnD",
        "outputId": "17277a44-0a85-43fe-8306-2788d590d6f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "float(a[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRh9xUE51FpC",
        "outputId": "0e4095ad-86c4-4ee8-997e-ddf44694284b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0].detach().numpy()#提取张量a的第一个元素并将其转换为一个不跟踪梯度的Numpy数组"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RHmhlMxnnF",
        "outputId": "06fb14f2-2e0f-488e-9e68-87bb607769f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[1] = 2.0#更改张量中第二个元素的值\n",
        "a"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl-vnqT2xnnH"
      },
      "source": [
        "We can construct a tensor directly from data, by passing a Python list to the constructor（我们可以直接通过向构造函数传递一个Python列表来从数据中构建一个张量）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp0lEVd0xnnI",
        "outputId": "16ad2d1c-ba16-4024-f69a-b1b8eb64da9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([5.5, 3, 6, 10])#直接构建一个1D张量\n",
        "print(points)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.5000,  3.0000,  6.0000, 10.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOV1YuC7xnnK"
      },
      "source": [
        "This has the same result as the code below（下面的代码会有相同的结果）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EXlVZHfxnnL",
        "outputId": "f314adc6-1a2f-4f4b-e406-067f04904720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.zeros(4) # <1>创建一个全为0，包含4个元素的1D张量\n",
        "#为每个元素赋值\n",
        "points[0] = 5.5 # <2>\n",
        "points[1] = 3.0\n",
        "points[2] = 6.0\n",
        "points[3] = 10.0\n",
        "print(points)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.5000,  3.0000,  6.0000, 10.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cVj548QxnnM"
      },
      "source": [
        "or create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user\n",
        "\n",
        "或者基于一个已有的张量创建一个新的张量。这些方法会重新使用输入张量的属性，例如数据类型（dtype），除非用户提供了新的值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1NphnstxnnN",
        "outputId": "32723db0-3d72-4d22-98d0-ac176cf8d5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
        "#new_ones()基于现有的张量创建一个新的张量，该张量的形状由参数决定，为5行3列\n",
        "#新张量的所有元素都是1\n",
        "#制定了新张量的数据类型为双精度浮点数:dtype=torch.double\n",
        "#这里使用x的new_ones()方法一位置新张量将继承x的一些属性，但'dtype'属性被显式覆盖了\n",
        "print(x)\n",
        "\n",
        "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
        "#利用.randn_like()方法创建一个新的张量，其形状与现有张量x相同，但是元素是从标准正态分布（均值为0，方差为1）中随机抽取的\n",
        "#将数据类型显式的指定为了单精度浮点数，dtype参数确保了新张量会覆盖掉原来的双精度属性\n",
        "print(x)                                      # result has the same size\n",
        "#结果具有相同的形状"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[-1.6708,  0.1006,  0.6313],\n",
            "        [-1.0381, -0.1398,  0.8454],\n",
            "        [-2.5629, -0.4833,  0.2527],\n",
            "        [ 0.2486,  1.1127, -1.3411],\n",
            "        [ 0.6846,  1.5196,  0.1718]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpdQILPxnnO",
        "outputId": "28e16329-9f63-4f70-b665-21acec3b883a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x.size())#重新输出张量的形状"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxnlG_CMxnnQ"
      },
      "source": [
        "Python lists or tuples of numbers are collections of Python objects that are individually allocated in memory. PyTorch tensors or NumPy arrays on the other hand are views over (typically) contiguous memory blocks containing  C numeric types unboxed rather than Python objects. For instance, float32 datatypes would each consist of 32-bit (4 byte) IEEE floating point values. This means that a 1D tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes to be stored, plus a small overhead for the meta data (e.g. dimensions, numeric type)\n",
        "\n",
        "Python列表或元组中的数字是单独分配在内存中的Python对象集合。另一方面，PyTorch张量或NumPy数组是（通常是）连续内存块的视图，这些内存块包含未封装的C数值类型而不是Python对象。例如，float32数据类型将包含32位（4字节）的IEEE浮点数值。这意味着一个包含1,000,000个浮点数的1D张量将确切需要4,000,000字节的连续字节来存储，加上一小部分元数据（例如，维度、数值类型）的开销。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apoNXwtxnnQ"
      },
      "source": [
        "What if we want to refer to 2D points? We can create a 2D tensor（如果我们想要引用2D的点，我们可以创建一个2D张量）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRZghDfxnnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ac37fe-1090-42d8-ec34-6304d7fa2ebd"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])#创建一个2D张量\n",
        "points"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZk2XbklxnnT"
      },
      "source": [
        "and you can check the shape of a tensor（可以确定张量的形状）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GtP4LElxnnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dbbeb0-db1a-453a-cc90-e37540637174"
      },
      "source": [
        "points.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc_YfvA3xnnX"
      },
      "source": [
        "You can also use zeros or ones to initialise the tensor, giving it a specific shape\n",
        "\n",
        "您也可以使用zeros或ones函数来初始化张量，并给它指定一个特定的形状。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVow0LVQxnnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c91e34-bb38-4956-d6bd-4b4644ed5117"
      },
      "source": [
        "points = torch.zeros(3, 2)\n",
        "points"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgxVb5cGLVP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48e1ee5-c641-4cd6-8a45-89875fbf93be"
      },
      "source": [
        "points = torch.ones(3,2)\n",
        "points"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcHvm_SKLb38"
      },
      "source": [
        "By default, tensors are created using datatype torch.float32 (32-bit floating point numbers). However, different datatypes for the elements can be specified with the dtype argument, or there are also methods that create tensors with a specific datatype.\n",
        "\n",
        "默认情况下，张量是使用数据类型torch.float32（32位浮点数）创建的。然而，可以通过dtype参数指定元素的不同数据类型，或者也可以使用创建具有特定数据类型张量的方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKqgeOxFQ3Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a694d81d-b3e3-4120-cb47-b35f2a707fd3"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], dtype=torch.float32)\n",
        "#创建一个2D张量，并制定张量的数据类型为32位浮点数\n",
        "print(points)\n",
        "print(points.dtype)#输出张量的数据类型"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cOioU7Exnna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de80ba61-9544-4e16-ad0d-def2805f3516"
      },
      "source": [
        "points = torch.FloatTensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "#与上一个单元格的代码相同，同样也是创建一个2D张量，FloatTensor预设了张量的数据类型为32位浮点数，所以不需要显示指定'dtype'\n",
        "print(points)\n",
        "print(points.dtype)#输出张量的数据类型"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgbDsiuGxnnc"
      },
      "source": [
        "If we wanted the $y$ coordinate of the 0th point（如果想要第0个点的$y$坐标）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93PmXk5xnnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376210c3-9b04-4940-e0e9-9be92c10454e"
      },
      "source": [
        "points[0, 1]#原本第0个点的坐标为(4.0,1.0)这样直接索引到y坐标，所以只输出tensor(1.)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL9NIUbtxnne"
      },
      "source": [
        "or just get the full 2D coordinates of the 0th point（反正只是获取第0个点的完整2D坐标）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Wtv--Cxnnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3536c7a5-7ed7-478a-a596-4b8f38eb6a6d"
      },
      "source": [
        "points[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBPzCh_xnnh"
      },
      "source": [
        "***Views on storage***\n",
        " Values in Tensors are allocated in contiguous chunks of memory, managed by `torch.Storage` instances. A storage is a one-dimensional array of numerical data, i.e. a contiguous block of memory containing numbers of a given type, such as `float`, 32-bits representing a floating point number, or `int64`, 64-bits representing an integer. A PyTorch  `Tensor` is a view over such a  `Storage` that is capable of indexing into that storage using an offset and and per-dimension strides.\n",
        "\n",
        " 对存储的视图：张量中的值是由'torch.Storage'实例管理的，分配在连续的内存块中。存储(Storage)是一种一维的数值数据数组，即一个包含给定类型数值的连续内存块，如float，32位表示一个浮点数，或int64，64位表示一个整数。一个PyTorch张量是一个对应这样存储的视图，它能够使用一个偏移量和每个维度上的步长来索引进这个存储。\n",
        "\n",
        "\n",
        "\n",
        " Multiple tensors can index the same storage, even if they index into the data differently. When we requested  `points[0]` above, what we got back is another tensor that indexes the same storage as the  tensor, just not all of it and points with different dimensionality (1D vs 2D).  The underlying memory is allocated only once, however, so creating alternate tensor-views on the data can be done quickly, no matter the size of the data managed by the `Storage` instance.\n",
        "\n",
        " 即使它们以不同的方式索引进数据，多个张量也可以索引同一个存储。当我们上面请求points[0]时，我们得到的是另一个索引了同一存储的张量，只是没有全部索引，它指向具有不同维度的数据（1D对比2D）。然而，底层内存只分配了一次，因此在存储实例管理的数据上创建交替的张量视图可以很快完成，无论数据的大小如何。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "所以利用point[0]提取出来的不是原本的张量，虽然仍指向原始存储，但是是一个新的张量，其中包含了原始张量的一部分（也就是第一个维度），意味着原始数据不会被复制，而是通过新的视图（多维张量）来表示"
      ],
      "metadata": {
        "id": "vcG9PklyKdBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "多个张量可以索引同一个存储，例如一个存储的子集的视图sub_tensor的存储sub_tensor.storage()将显示它的全集（也就是原始数据的存储）"
      ],
      "metadata": {
        "id": "2HhEqDaoM4yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "所谓的偏移量(offset)和步长(stride)就是在取数据的时候从哪个位置开始取，并且怎么取，可以根据设置的步长跳着取）"
      ],
      "metadata": {
        "id": "gc9-cUqrKuJ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQznXczcxnnh"
      },
      "source": [
        "**Indexing into storage（索引进存储）** Let’s see how indexing into the storage works in practice with our 2D points. The storage for a given tensor is accessible using the `.storage` property（让我们通过实践来了解如何对我们的2D点进行存储索引。可以使用.storage属性来访问给定张量的存储）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnAwQsqAxnni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c467c5-59fa-441e-e037-a3dba8a86360"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "#这里的存储是一个torch.FloatStorage，大小为6，包含了张量所有元素的值\n",
        "#输出将会按照在内存中的存储顺序排量，表明虽然'points'是一个2D张量，但是其底层数据在物理内存中是连续存储\n",
        "#说明在底层存储中的存储方式是连续的一维数组\n",
        "points.storage()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-1fa12a19e062>:5: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points.storage()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZfNmQM-xnnk"
      },
      "source": [
        "Even though the tensor reports itself as having 3 rows and 2 columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows how to translate a pair of indices into a location in the storage. We can also index into a storage manually, for instance:\n",
        "\n",
        "尽管张量自报为有3行2列，但底层的存储实际上是一个大小为6的连续数组。从这个意义上讲，张量只是知道如何将一对索引转换为存储中的一个位置（因为实际上在内存中可能是连续存储的，并不是存在真正的索引位置）。我们也可以手动对存储进行索引，例如："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej234D9Txnnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f7b31e-8d1b-4a10-f6e1-68493804b2fa"
      },
      "source": [
        "points_storage = points.storage()\n",
        "print(points_storage[0])#输出第0个索引对应的元素\n",
        "print(points.storage()[1])#输出第一个索引对应的元素\n",
        "#输出的都是存储在内存上的值，而不是张量了"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9sT6kjxnnl"
      },
      "source": [
        "If you have a one element tensor, use .item() to get the value as a Python number\n",
        "\n",
        "如果你有一个只包含一个元素的张量，使用 .item() 来获取作为Python数值的值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_SB7up0xnnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2155c5bf-8e21-486d-fa7e-67fc2cfb8b49"
      },
      "source": [
        "print(points[0,1].item())#相当于将张量中对应位置的元素转换为Python的浮点数"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnBQVYcxnno"
      },
      "source": [
        "We can’t index a storage of a 2D tensor using two indices. The layout of a storage is always one-dimensional, irrespective of the dimensionality of any and all tensors that might refer to it.\n",
        "\n",
        "我们不能使用两个索引来索引一个2D张量的存储。存储的布局始终是一维的，不论引用它的任何张量的维度如何。\n",
        "\n",
        "Changing the value of a storage leads to changing the content of its referring tensor（更改存储的值将导致引用它的张量的内容发生变化）:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWEyL_3xnnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2671323a-e8b3-4f6c-e80b-22f6b0077d42"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_storage = points.storage()\n",
        "points_storage[0] = 2.0#更改了存储在第一个位置上的数据的值\n",
        "points#导致相应的张量也发生了变化\n",
        "#说明可以直接修改存储来改变引用它的张量的内容，说明了张量和其存储之间的紧密关联"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLsg-upZxnnq"
      },
      "source": [
        "In order to index into a storage, tensors rely on a few pieces of information, which, together with their storage, unequivocally define them: size, storage offset and stride.  The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. The storage offset is the index in the storage corresponding to the first element in the tensor. Stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n",
        "\n",
        "为了对存储进行索引，张量依赖于几个信息，这些信息连同它们的存储一起，明确地定义了它们：大小（size）、存储偏移量（storage offset）和步长（stride）。大小（或在NumPy术语中称为形状shape）是一个元组，指示张量在每个维度上表示的元素数量。存储偏移量是存储中对应于张量中第一个元素的索引。步长是为了获得每个维度上的下一个元素需要在存储中跳过的元素数量。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "大小（Size/Shape）：告诉我们张量在每个维度上有多少个元素，例如，一个3x2的张量的大小是(3, 2)。\n",
        "\n",
        "存储偏移量（Storage Offset）：指明在底层存储中，张量的第一个元素相对于存储开始位置的偏移。这允许张量视图共享同一个存储，而表示不同的数据切片。\n",
        "\n",
        "步长（Stride）：一个元组，表示在每个维度上，从一个元素移动到下一个元素需要跳过的存储中的元素数量。步长决定了张量的布局（如是连续的、是行优先还是列优先等）以及如何快速地计算多维索引在一维存储中的位置。"
      ],
      "metadata": {
        "id": "ScfCdrP6X329"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9H5tEVkxnnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06307db-07b8-42c4-f85e-65bd0dafc5ce"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "second_point = points[1]#取张量中的第二个点赋给变量second_point\n",
        "print(second_point.storage_offset())#打印second_point的存储偏移量\n",
        "#存储偏移量是指'second_point'在'points'的底层存储中第一个元素的位置\n",
        "#因为在底层是按照数组的连续存储，所以'second_point'的第一个元素实际上是存储的第三个元素（索引为2）开始选的，所以偏移量为2（对应的元素索引是2（0->2）)\n",
        "\n",
        "second_point.size(), second_point.shape\n",
        "#.size()和.shape都是查询'second_point的维度的恶，这将返回其在每个维度上的元素数量\n",
        "#因为是1D张量，所以Size类中只包含了一个元素，但这个维度包含了两个元素，所以是2"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS2mXI67xnns"
      },
      "source": [
        "The resulting tensor has offset 2 in the storage (since we need to skip the first point, which has two items) and the size is an instance of the `Size` class containing one element, since the tensor is one-dimensional. Important note: this is the same information as contained in the `shape` property of tensor objects:\n",
        "\n",
        "生成的张量在存储中的偏移量为2（因为我们需要跳过第一个点，它包含两个元素），并且大小（size）是一个 Size 类的实例，其中包含一个元素，因为张量是一维的。重要说明：这与张量对象的 shape 属性中包含的信息相同："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTUCNbgtxnnt"
      },
      "source": [
        "Last, stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. For instance, our `points`  tensor has a stride of : (2, 1)\n",
        "\n",
        "最后，步长（stride）是一个元组，指示在每个维度上，当索引增加1时需要跳过多少个存储中的元素。例如，我们的 points 张量具有以下步长：(2, 1)。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这意味着在第一个维度上（行），当索引增加1时，需要跳过2个元素；在第二个维度上（列），当索引增加1时，需要跳过1个元素。步长的定义允许张量视图可以有效地定位存储中的元素，即使底层存储是连续的一维排列。"
      ],
      "metadata": {
        "id": "Fv5qlDODa6zu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ihnx4YBxnnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b9d2e8-f646-44e9-aaed-be973770f46a"
      },
      "source": [
        "points.stride()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里步长为(2,1)的原因是在第一个维度上，从第一个点（第一行）移动到下一个点（第二行）实际是要跳过两个元素，因为在内部存储中，x坐标和y坐标被存储在连续的位置，所以需要跳过一个x坐标和一个y坐标，所以第一个维度步长为2。\n",
        "\n",
        "而在第二个维度，也就是一个点的内部，x和y坐标本身是连续的，所以在内部存储中没有额外的元素需要跳过）"
      ],
      "metadata": {
        "id": "RnREWeGGb9Vd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5AV3s8Axnnv"
      },
      "source": [
        "This indirection between a tensor and its storage leads to some operations, like transposing a tensor or extracting a sub-tensor, to be inexpensive, as they do not lead to memory reallocations; instead they consist in allocating a new tensor object with a different value for size, storage offset or stride.\n",
        "\n",
        "张量与其存储之间的这种间接关系导致一些操作（例如转置张量或提取子张量）具有低成本，因为它们不会导致内存重新分配；相反，它们仅涉及分配一个具有不同大小、存储偏移或步长值的新张量对象。（这意味着在执行这些操作时，不会实际复制或移动数据，而只是创建一个新的张量对象，该对象引用相同的存储，但具有不同的视图。这提高了操作的效率，并且在处理大型数据集时尤为重要，因为它避免了不必要的内存开销和数据复制。）\n",
        "\n",
        "Let’s try with transposing now. Let’s take our  tensor, that has individual points in the points rows and x and y coordinates in the columns, and turn it around so that individual points are along the columns. We take this opportunity to introduce the  function, a short-hand alternative t for 2-dimensional tensors\n",
        "\n",
        "让我们尝试进行转置操作。我们有一个张量，其中每个点位于行中，而x和y坐标位于列中，现在我们要将其转置，使得每个点位于列中。我们利用这个机会介绍一个函数，即用于表示二维张量的简写t。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLmwrN-0xnnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d493492a-64fa-402a-c59b-b610094c6f2f"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO-A4W0Kxnnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd00a30-dc80-4202-f863-e5c15f435c44"
      },
      "source": [
        "points_t = points.t()#对初始张量进行转置操作，行列进行转换\n",
        "points_t"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZIf7oTxnny"
      },
      "source": [
        "We can easily verify that the two tensors share the same storage（我们可以很容易地验证这两个张量共享相同的存储空间）:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD0NzKWfxnnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a07da45-47ac-4c96-fb14-af3a5af28911"
      },
      "source": [
        "id(points.storage()) == id(points_t.storage())#用于检查两个张量的存储空间是否相同\n",
        "#id()返回一个对象的唯一标识符，这个不标识符在这个对象的生命周期内保持不变\n",
        "#如果两个张量共享相同的存储空间，则他俩的唯一标识符也应该相同\n",
        "print(id(points.storage()) == id(points_t.storage()))#理论上共享相同的存储空间应该返回'true'\n",
        "points.storage()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points_t.storage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bVVr-P-I0hx",
        "outputId": "e35af33a-4fa2-4e88-c091-0e6e66b714fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE5vh50Nxnn0"
      },
      "source": [
        "and that they differ only in the shape and stride（它们只在形状和步长上有所不同。）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaW9Fvy1xnn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214fdee6-f686-4483-ba94-e1565ddecede"
      },
      "source": [
        "points.stride()#初始张量的步长为(2,1)\n",
        "#在行维度上跨越了两个内存位置到达第二行\n",
        "#列维度因为每个元素都是相邻的，只需要跨越1个内存位置，所以步长为1"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt3L7h4Nxnn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc40d6f9-7957-4b0e-9f72-44f40d5c8e35"
      },
      "source": [
        "points_t.stride()\n",
        "#在行维度上只需要跨越1个内存位置就可以到达下一个元素，因为在行上是相邻的\n",
        "#转置后的张量在列上需要跨越两个元素才能到达第二列(4.0->5.0)\n",
        "#因为在内存中的存储顺序是不变的，参照上面的.storage()方法的输出"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77h284bwxnn4"
      },
      "source": [
        "Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional array by specifying the two dimensions along which transposing (i.e. flipping shape and stride) should occur\n",
        "\n",
        "在PyTorch中的转置不仅限于矩阵。我们可以通过指定应该发生转置（即翻转形状和步长）的两个维度来转置多维数组。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGhi_PYcxnn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5753f2f-b248-4ea2-d3c6-870d8291af87"
      },
      "source": [
        "some_t = torch.ones(3, 4, 5)#创建一个3维张量，所有的元素都是1，形状为(3,4,5)\n",
        "transpose_t = some_t.transpose(0, 2)#在第0维和第2维之间进行转置，交换这两个维度的大小\n",
        "some_t.shape\n",
        "#初始张量的大小如下，代表张量的深度为3层，每层张量的大小为4x5\n",
        "#第一个维度大小为5，第二个维度为4"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelLEqtzxnn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fbcc6f-fb54-41ef-eb45-58aeacdc50a7"
      },
      "source": [
        "transpose_t.shape\n",
        "#转置后张量的大小如下，代表张量的深度变成了5层，第二个维度为4，最内层第一个维度为3"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4kxKI1xnn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b944971-a67f-4fec-97e7-3520cca05375"
      },
      "source": [
        "some_t.stride()\n",
        "#在最内层维度只需要跨越一个内存位置，就可以到下一个元素，因为在最内存维度，元素是连续存储的\n",
        "#在第二个维度，从一个元素，移动到下一个元素需要跨越5个内存位置，相当于在最内存维度，一行有5个元素\n",
        "#在最外层维度，从一个元素移动到下一个元素需要跨越20个元素，因为相当于一个张量面有20（5x4）个元素"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 5, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRus_QGkxnn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5baf866a-99ec-4fe3-eba7-9534be8f76c6"
      },
      "source": [
        "transpose_t.stride()\n",
        "#将张量进行转置后，最内层维度与最外层维度进行转换，但是其实内存中元素的位置是不变的"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6jYO6bxnoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979e42b3-531d-4d73-a505-955155f65c1b"
      },
      "source": [
        "points.is_contiguous()#.is_contiguous()用来检查张量在内存中是否是连续存储的\n",
        "#返回true说明是连续存储的"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyx6dUKKxnoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a8e6a5-5083-462b-c5f9-1de1423fcd02"
      },
      "source": [
        "points_t.is_contiguous()#因为'points_t'是经过转置操作得到的，因为转置操作改变了原始张量的形状和步长，所以会导致新的张量在物理内存中不连续\n",
        "#元素的逻辑顺序与它们在内存中的物理存储顺序不匹配"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av3xEzW1xnoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04a7b73-c80f-44ce-c8ac-246a89185237"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_t = points.t()\n",
        "points_t"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfbWAIZKxnoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3151480-6f13-433a-9d84-25309dba1be5"
      },
      "source": [
        "points_t.storage()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ZBtfIhxnoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32443c6e-1cf7-412e-a7ed-210270cf9b2e"
      },
      "source": [
        "points_t.stride()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF0DEt_nqlYY"
      },
      "source": [
        "The contiguous() method returns a tensor which has had memory reallocated so that the data elements are contiguous in memory.\n",
        "\n",
        "contiguous()方法返回一个经过内存重新分配的张量，使得数据元素在内存中连续存储。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnfiUZnMxnoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236244f3-bc84-493f-9ac3-86c7287fea34"
      },
      "source": [
        "points_t_cont = points_t.contiguous()#用来确保'points_t'张量在内存中是连续存储的\n",
        "#如果原本的points_t不是连续的，则contiguous()将创建一个新的张量，来在内存中连续存储points_t的数据\n",
        "points_t_cont#是一个在内存中连续存储的张量"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbWx-l92xnoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc06c32f-4d3f-4d60-9a5c-d21660c15f7b"
      },
      "source": [
        "points_t_cont.stride()#步长为(3,1)这取决于张量在内存中的元素存储顺序"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6WFYUPDxnoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7cc6ee-1c0b-4a00-8739-0eeceb9cabb3"
      },
      "source": [
        "points_t_cont.storage()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 5.0\n",
              " 2.0\n",
              " 1.0\n",
              " 3.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RDto5UcrZkW"
      },
      "source": [
        "Tensors of different datatypes can be allocated by specifying the `dtype` attribute.\n",
        "\n",
        "通过指定dtype属性，可以分配不同数据类型的张量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evoMGCtaxnoS"
      },
      "source": [
        "double_points = torch.ones(10, 2, dtype=torch.double)\n",
        "#创建一个形状为(10,2)的张量，所有元素都是1，指定张量的数据类型为double双精度浮点数，通常是64位，会占据更多的内存空间\n",
        "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
        "#创建一个形状为(2,2)的张量，初始化相应的值，并指定张量的数据类型为short短整型，short表示较小范围的整数，占用的内存空间小，通常是16位"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJCnDOIXxnoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f01b723-5acb-4891-a805-ecd9ea38c0de"
      },
      "source": [
        "print(double_points.dtype)\n",
        "print(short_points.dtype)\n",
        "#分别输出两个张量的数据类型"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.int16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AStk5MKWr4OY"
      },
      "source": [
        "Alternatively, methods such as `double()` can be used to convert the datatype of a current tensor.\n",
        "\n",
        "另外，可以使用double()等方法来转换当前张量的数据类型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sIPa7sSxnoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95bac0c-a69e-4777-b092-df0e2a358fd9"
      },
      "source": [
        "double_points = torch.zeros(5, 2).double()#指定张量的元素的数据类型为双精度浮点数\n",
        "short_points = torch.ones(5, 2).short()#指定张量元素的数据类型为短整型\n",
        "print(double_points)\n",
        "print(short_points)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]], dtype=torch.float64)\n",
            "tensor([[1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQUjGTissLll"
      },
      "source": [
        "The `to()` method can also be used to convert tensors to a particular torch datatype.\n",
        "\n",
        "to()方法也可以用来将张量转换为特定的torch数据类型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cszxvax-xnoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf0c57f-20bf-4c55-ee67-9ec6169473d6"
      },
      "source": [
        "double_points = torch.zeros(5, 2).to(torch.double)#将张量的数据类型转换为双精度浮点数\n",
        "short_points = torch.ones(5, 2).to(dtype=torch.short)#将张量的数据类型转换为短整型\n",
        "print(double_points)\n",
        "print(short_points)\n",
        "#输出相应的张量"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]], dtype=torch.float64)\n",
            "tensor([[1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ4eN3Qqs135"
      },
      "source": [
        "And finally the method `type()` can also be used to convert tensors to a particular type.\n",
        "\n",
        "最后，type()方法也可以用来将张量转换为特定的类型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USWZzSKTxnoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6545f644-bec2-4831-9f9c-0f83b26f2502"
      },
      "source": [
        "points = torch.randn(5, 2)#创建一个随机的浮点数张量points\n",
        "#points的形状为(5,2),其中的元素是从标准正态分布(均值为0，方差为1)中随机抽取的浮点数\n",
        "#5行2列的随机数\n",
        "print(points)\n",
        "short_points = points.type(torch.short)#将初始张量的数据类型从默认的浮点数转换为短整型short\n",
        "#type()可以转换张量的护具类型，这里转换为短整型导致浮点数值会被截断为整数，会丢失精度\n",
        "print(short_points)#输出截断后的整数值"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.9753,  0.4367],\n",
            "        [-0.1200, -0.0731],\n",
            "        [ 0.1681,  1.5561],\n",
            "        [-0.9650, -0.3175],\n",
            "        [ 0.3041,  0.7051]])\n",
            "tensor([[-1,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  1],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjhjIMUGxnoa"
      },
      "source": [
        "# reset points back to original value#重新将points张量指定会原始数值\n",
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3_jV140xnob"
      },
      "source": [
        "***Indexing Tensors***\n",
        "What if we need to obtain a tensor containing all points but the first? That’s easy using range indexing notation, the same that applies to standard Python lists, which we quickly recall. (Add print statements to the other entries if you are unsure what they will produce as output.)\n",
        "\n",
        "**张量索引：**如果我们需要获得一个包含除了第一个点以外的所有点的张量，该怎么办？使用范围索引符号很容易做到，这与标准Python列表的符号相同，我们很快就可以回忆起来。（如果你不确定它们会产生什么输出，可以添加打印语句到其他条目。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93Yu9OJxnob",
        "outputId": "e8b381d2-c5c0-4bd0-859d-f63137341f93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "some_list = list(range(6))\n",
        "print(some_list)\n",
        "\n",
        "some_list[:]            # <1>\n",
        "print(some_list[:])\n",
        "\n",
        "some_list[1:4]          # <2>\n",
        "print(some_list[1:4])\n",
        "\n",
        "some_list[1:]           # <3>\n",
        "print(some_list[1:])\n",
        "\n",
        "some_list[:4]           # <4>\n",
        "print(some_list[:4])\n",
        "\n",
        "some_list[:-1]          # <5>\n",
        "print(some_list[:-1])#输出从开始到倒数第二个元素，-1表示最后一个元素，所以不包含\n",
        "\n",
        "print(some_list[1:4:2]) # <6>"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n",
            "[0, 1, 2, 3, 4, 5]\n",
            "[1, 2, 3]\n",
            "[1, 2, 3, 4, 5]\n",
            "[0, 1, 2, 3]\n",
            "[0, 1, 2, 3, 4]\n",
            "[1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEqs-Z-9xnoc"
      },
      "source": [
        "1. all elements in the list\n",
        "2. from element 1 inclusive to element 4 exclusive\n",
        "3. from element 1 inclusive to the end of the list\n",
        "4. from the start of the list to element 4 exclusive\n",
        "5. from the start of the list to one before the last element\n",
        "6. from element 1 inclusive to element 4 exclusive in steps of 2\n",
        "\n",
        "1.列表中的所有元素\n",
        "\n",
        "2.从第1个元素（包含）到第4个元素（不包含）\n",
        "\n",
        "3.从第1个元素（包含）到列表的末尾\n",
        "\n",
        "4.从列表的开始到第4个元素（不包含）\n",
        "\n",
        "5.从列表的开始到最后一个元素的前一个元素\n",
        "\n",
        "6.从第1个元素（包含）到第4个元素（不包含），步长为2\n",
        "\n",
        "To achieve our goal we can use the same notation for PyTorch tensors, with the added benefit that, just like in NumPy and in other Python scientific libraries, we can use range indexing for each of the dimensions of the tensor:\n",
        "\n",
        "为了实现我们的目标，我们可以对PyTorch张量使用相同的符号，而且像在NumPy和其他Python科学库中一样，我们可以对张量的每个维度使用范围索引：\n",
        "\n",
        "(Again add print statements if you are uncertain what values are produced by any of these statements.)\n",
        "\n",
        "如果你不确定这些语句会产生什么值，再次添加打印语句。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1OTMx3xnod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b678ec7-251e-4450-fa83-f91cb82bf5b3"
      },
      "source": [
        "print(points)\n",
        "\n",
        "points[1:]          # <1>\n",
        "print(points[1:])\n",
        "\n",
        "points[1:, :]       # <2>\n",
        "print(points[1:, :])#输出从第二行开始的所有行，以及所有列，也就是从第二行到最后一行的所有数据\n",
        "\n",
        "points[1:, 0]       # <3>\n",
        "print(points[1:, 0])#输出从第二行到最后一行的所有行以及第一列\n",
        "#对应的结果是一个一维张量（向量），包含了points张量的第一列的第二个元素到最后一个元素\n",
        "\n",
        "print(points[None]) # <4>"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([5., 2.])\n",
            "tensor([[[4., 1.],\n",
            "         [5., 3.],\n",
            "         [2., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsHejolxnoe"
      },
      "source": [
        "1. All rows after first, implicitly all columns\n",
        "2. All rows after first, all columns\n",
        "3. All rows after first, first column\n",
        "4. Add dimension of size one, just like unsqueeze (note the extra square brackets printed compared to just points)\n",
        "\n",
        "1.第一行之后的所有行，隐含选择所有列\n",
        "\n",
        "2.第一行之后的所有行，所有列\n",
        "\n",
        "3.第一行之后的所有行，第一列\n",
        "\n",
        "4.增加一个大小为一的维度，就像unsqueeze方法一样（注意与仅仅是points相比，打印出来的额外的方括号）"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`unsqueeze`方法是PyTorch中的一个张量操作，它的作用是在张量的指定位置增加一个维度，也就是增加一个大小为1(在外层增加一个括号)的维度。这通常用于调整张量的形状，以便能够进行某些特定的操作，比如在进行广播（broadcasting）操作或者准备批处理数据时。"
      ],
      "metadata": {
        "id": "XGJ-bjOe7Ypy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unsqueeze方法\n",
        "x = torch.randn(3, 4)  # 创建一个形状为(3, 4)的张量\n",
        "x_unsqueezed_0 = x.unsqueeze(0)  # 在第0维增加一个维度，使形状变为(1, 3, 4)\n",
        "print(x_unsqueezed_0)\n",
        "\n",
        "x_unsqueezed_1 = x.unsqueeze(1)  # 在第1维增加一个维度，使形状变为(3, 1, 4)\n",
        "print(x_unsqueezed_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpfAAdx87FBG",
        "outputId": "b1ea2c1b-c63c-4a87-aa33-b0e289677265"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.2563,  0.3172, -0.6943,  0.4873],\n",
            "         [ 0.8684,  1.4021, -0.4583,  0.8836],\n",
            "         [-2.1082,  0.7872, -0.8202, -1.2178]]])\n",
            "tensor([[[-1.2563,  0.3172, -0.6943,  0.4873]],\n",
            "\n",
            "        [[ 0.8684,  1.4021, -0.4583,  0.8836]],\n",
            "\n",
            "        [[-2.1082,  0.7872, -0.8202, -1.2178]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-b_u-Jpxnoe"
      },
      "source": [
        "***Named Tensors***\n",
        "The dimensions (or axes) of our Tensors usually index something like pixel locations or color channels. This means that when we want to index into our Tensor, we need to remember the ordering of the dimensions and write our indexing accordingly. As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone.\n",
        "To make things concrete, imagine that we have a 3D Tensor like `img_t` (we will use dummy data for simplicity here) and want to convert it to grayscale. We looked up typical weights for the colors to derive a single brightness value\n",
        "\n",
        "**命名张量：**我们的张量的维度（或轴）通常索引的是像素位置或颜色通道之类的东西。这意味着当我们想要索引进入我们的张量时，我们需要记住维度的顺序并相应地编写我们的索引。当数据通过多个张量转换时，跟踪哪个维度包含了哪些数据可能会容易出错。为了具体化，假设我们有一个像img_t这样的3D张量（这里为了简单起见我们使用虚拟数据），并且想要将其转换为灰度图。我们查找了颜色的典型权重，以便推导出单一的亮度值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuj59ECQxnoh"
      },
      "source": [
        "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
        "#创建了一个形状为(3,5,5)的张量，代表了一个具有3个颜色通道（一般为红、绿、蓝)和5x5像素的图像\n",
        "#对应的形状每个参数代表:[通道数，行，列]\n",
        "#其中每个元素都是从标准正态分布(均值为0，方差为1)中随机抽取的\n",
        "#img_t中的每个值都是随机的，模拟了一个随机图像的像素值\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
        "#weights创建了一个包含了3个元素的一维张量，三个值表示转换彩色图像到灰度图像时，红色、绿色和蓝色通道权重\n",
        "#使用这些权重将彩色图像的每个像素的颜色值组合成单一的亮度值，可以得到相应的灰度图像。\n",
        "#可以讲随机的彩色图像转换为灰度图像\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJa3p4lzxnoj"
      },
      "source": [
        "We also often want our code to generalize - for example from grayscale images represented as 2D Tensors with height and width dimensions to color images adding a third channel dimension (as in RGB) or from a single image to a batch of images.\n",
        "\n",
        "我们还经常希望我们的代码具有泛化性——例如，从表示为具有高度和宽度维度的2D张量的灰度图像，泛化到添加了第三个通道维度（如RGB）的彩色图像，或从单个图像泛化到一批图像。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "批次batch是指在一次训练中同时处理一组数据样本，一个批次通常包含多个图像数据，可以同时在多个图像上进行操作，进行并行操作，批次大小对应了每次处理的图像的数量"
      ],
      "metadata": {
        "id": "B92i62Hv9QWT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hb7EqYQxnok"
      },
      "source": [
        "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
        "#生成一个4维张量，每个元素都是随机数，形状为[批次大小，通道数，行，列]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCut_4NFxnom"
      },
      "source": [
        "So sometimes the RGB channels are in dimension 0 and sometimes in dimension 1. But we can generalize by counting from the end: They are always in dimension -3, the third from the end. The lazy, unweighted mean would thus be written as follows:\n",
        "\n",
        "因此，有时RGB通道位于维度0，有时位于维度1。但我们可以通过从末尾开始计数来进行泛化：它们总是位于倒数第三个维度，即维度-3。因此，懒惰的、未加权的平均值可以如下编写（RGB通道维度总是位于倒数第三个维度！！）：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilAe0YwExnom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec57e11-a54b-46ba-a6b2-2ba0915baeb2"
      },
      "source": [
        "img_gray_naive = img_t.mean(-3)\n",
        "#计算了img_t张量沿着倒数第三个维度（颜色通道维度）的平均值，结果是一个灰度图像，其中每个像素的值是原始颜色的平均\n",
        "#这样会导致减少一个维度，张量形状会变成[rows,columns]\n",
        "batch_gray_naive = batch_t.mean(-3)\n",
        "#对batch_t张量执行相同的操作，同样沿着颜色通道维度计算平均值，每个图像批次中的所有图像都被转换为灰度，最后的形状为:[批次大小，rows,columns]\n",
        "img_gray_naive.shape, batch_gray_naive.shape\n",
        "#分别返回单个图像和图像批次转换为灰度后的形状"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对三个颜色通道的值取平均可已经原有的彩色信息简化为单一的亮度值，这个值可以大体地反应原始颜色的敏感程度"
      ],
      "metadata": {
        "id": "9QCw2lhVJGkp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFJOw4iFxnoo"
      },
      "source": [
        "But now we have the weight, too. PyTorch will allow us to multiply things that are of same shape, but also of shapes where one operand is of size one in a given dimension. It also appends leading dimensions of size one automatically. This is a feature called *broadcasting*. We see that our `batch_t` of shape (2, 3, 5, 5) gets multiplied with the `unsqueezed_weights` of shape (3, 1, 1) to a tensor of shape (2, 3, 5, 5), from which we can then sum the third dimension from the end (the 3 channels).\n",
        "\n",
        "但现在我们也有了权重。PyTorch允许我们乘以形状相同的东西，但也允许乘以在给定维度上其中一个操作数大小为一的形状（意思是如果在某些维度上其中一个张量的大小为1，这个维度会被自动扩展来匹配另一个张量相应维度的大小，例如(3,1,)这个张量在第二和第三维度的大小是1，那这两个维度就会被扩展来匹配另一个张量，从而进行元素记得乘法运算）。它还会自动在前面添加大小为一的维度（意思是如果两个张量的维数不一致，会在较小的维度张量前面自动添加维度，带下为1，来匹配两个张量的维数）。这是一种称为广播（*broadcasting*）的特性。我们看到，形状为(2, 3, 5, 5)的`batch_t`与形状为(3, 1, 1)的`unsqueezed_weights`相乘，得到一个形状为(2, 3, 5, 5)的张量，然后我们可以从这个张量中求和倒数第三个维度（3个通道）。\n",
        "\n",
        "Make sure you understand how the `unsqueeze()` method is working from the printed outputs (count the square brackets on the output!), and then also how the broadcasting is working when doing multiplications.\n",
        "\n",
        "确保你通过打印输出（数一数输出中的方括号！）理解了`unsqueeze()`方法是如何工作的，然后也要理解在进行乘法运算时广播是如何工作的。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsqueeze()方法就是用来给张量增加维度的方法，可以指定在那个维度上来增加"
      ],
      "metadata": {
        "id": "4T-mGicDKdpJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10LpIAIxnop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428e14d8-68b1-4557-cfab-18d6408dc2b7"
      },
      "source": [
        "#对图像进行加权灰度转换，包括单个图像和图像批次的处理\n",
        "print(f\"Shape of weights: {weights.shape}\")\n",
        "print(f\"Value: {weights} \\n\")\n",
        "\n",
        "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "#两次使用unsqueeze()方法，首先在最后一个维度(-1)添加一个大小为1的新维度，然后再在这个新维度上再增加一个大小为1的新维度\n",
        "#这样weights张量就从一维转成了三维形状变成了(3,1,1)使其可以进行后续的操作\n",
        "\n",
        "#输出经过unsqueeze后的张量形状和值\n",
        "print(f\"Shape of unsqueezed weights: {unsqueezed_weights.shape}\")\n",
        "print(f\"Value: {unsqueezed_weights}\\n\")\n",
        "\n",
        "\n",
        "img_weights = (img_t * unsqueezed_weights)#将单个图像张量与经过unsqueeze后的权重进行元素乘法，利用了广播机制，使得每个颜色通道的权重都可以被应用到img_t的对应颜色通道上\n",
        "batch_weights = (batch_t * unsqueezed_weights)#对图像批次同样进行相应的元素乘法，利用广播机制，将每个颜色通道的权重应用到每个图像的对应颜色通道上\n",
        "\n",
        "img_gray_weighted = img_weights.sum(-3)#沿着颜色通道维度对甲醛后的图像张量求和，得到加权灰度图像\n",
        "batch_gray_weighted = batch_weights.sum(-3)#同样沿着颜色通道维度来求和，得到相对应的加权灰度图像\n",
        "\n",
        "#分别输出加权灰度转换后的图像批次形状\n",
        "print(f\"Shape of img_gray_weighted: {img_gray_weighted.shape}\")\n",
        "print(f\"Shape of batch_gray_weighted: {batch_gray_weighted.shape}\")\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of weights: torch.Size([3])\n",
            "Value: tensor([0.2126, 0.7152, 0.0722]) \n",
            "\n",
            "Shape of unsqueezed weights: torch.Size([3, 1, 1])\n",
            "Value: tensor([[[0.2126]],\n",
            "\n",
            "        [[0.7152]],\n",
            "\n",
            "        [[0.0722]]])\n",
            "\n",
            "Shape of img_gray_weighted: torch.Size([5, 5])\n",
            "Shape of batch_gray_weighted: torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#执行第一次的unsqueeze(-1)的张量值，形状变为(3,1)是二维张量\n",
        "\"\"\"\n",
        "tensor([[0.2126],\n",
        "        [0.7152],\n",
        "        [0.0722]])\n",
        "\"\"\"\n",
        "#执行第二次的unsqueeze(-1)的张量值，形状变为(3,1,1)，是三维张量\n",
        "\"\"\"\n",
        "tensor([[[0.2126]],\n",
        "\n",
        "        [[0.7152]],\n",
        "\n",
        "        [[0.0722]]])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5UoQUoU_OI8E",
        "outputId": "8a1f4235-03bf-427e-b8d4-8144e7a0138e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntensor([[[0.2126]],\\n\\n        [[0.7152]],\\n\\n        [[0.0722]]])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "加权和灰度值和简单平均灰度值都是单个像素灰度值的表现方式"
      ],
      "metadata": {
        "id": "oxY1hyP1OyuT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6f4JN1xnoq"
      },
      "source": [
        "Because this gets messy quickly (and for efficiency), there even is a PyTorch function einsum (adapted from NumPy) that specifies an indexing mini-language  giving index names to 28 dimensions for sums of such products. As often in Python, broadcasting — a form of summarizing unnamed things — is done using three dots `...`\n",
        "\n",
        "因为这很快就会变得复杂（并且为了效率），PyTorch（从NumPy中借鉴）甚至提供了一个名为einsum的函数，该函数使用一个索引迷你语言，为28个维度的这类产品之和指定索引名称。像Python中经常出现的那样，广播——一种总结未命名事物的形式——使用三个点...来完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "einsum函数使用一种简短的索引语言来描述张量操作，通过给出索引名(通常使用字母表示)来指定如何对输入张量进行操作，einsum函数允许对最多28个维度的张量进行操作，意味着它可以处理非常高维的数据，同时在einsum函数中，可以使用...来表示广播操作，用来简化对多维张量进行操作的表达"
      ],
      "metadata": {
        "id": "GkoTVqqwNCRo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzMJKaBmxnor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6240bef-60a4-4629-d133-73a1b7347a81"
      },
      "source": [
        "#这段代码使用了einsum函数展示了如何来进行加权灰度转换，通过指定操作的索引表示法来计算\n",
        "\n",
        "img_gray_weighted_fancy   = torch.einsum('...chw,c->...hw', img_t, weights)\n",
        "#将单个图像的张量'img_t'转换为加权灰度图像\n",
        "\"\"\"\n",
        "其中第一个参数是一个索引表达式，其中：\n",
        "1.'...chw'表示img_t张量，c、h、w分别代表通道、高度、宽度的维度，而...代表可能存在的其他维度\n",
        "2.',c'表示weights张量，它只有一个维度，对应于img_t的通道维度。\n",
        "3.'->...hw'指定了输出张量的维度顺序，即除去通道维度，仅保留高度h和宽度w的维度。\n",
        "\"\"\"\n",
        "#利用einsum函数展现了两个张量进行元素乘法的过程\n",
        "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
        "#此行代码处理一个图像批次batch_t，这里...代表了批次大小这一个维度，使用相同的einsum索引表达式，可以直接应用于整个批次\n",
        "#对整个批次的所有的图像对应的张量与权重进行元素乘法，将每个图像转换为加权灰度图像\n",
        "\n",
        "#分别将转换后的单个图像和图像批次的灰度图像形状打印出来\n",
        "print(img_gray_weighted_fancy.shape)#对应输出形状为:[rows,columns],去除了RGB维度大小为3\n",
        "print(batch_gray_weighted_fancy.shape)#对应的输出形状为:[batch_size,rows,columns]，去除了RGB维度大小为3"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n",
            "torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDYvZZjxnou"
      },
      "source": [
        "PyTorch 1.3 added  as an experimental feature. Tensor factory functions such as `tensor`  or `rand` take a `names` argument. The names should be a sequence of strings\n",
        "\n",
        "PyTorch 1.3作为一个实验性功能添加了。诸如`tensor`或`rand`之类的张量工厂函数接受一个`names`参数。names应该是字符串序列。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3tgMq_dxnou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9607d2-9d7a-4d75-e074-c549c22de499"
      },
      "source": [
        "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
        "#创建一个具有命名维度的张量，通过制定'names=['channels]'参数，给张量的维度赋予了名称，该张量的每个元素应该对应不同的颜色通道的权重值\n",
        "#用于将RGB图像转换为灰度图像\n",
        "weights_named\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-d7b0e02475c2>:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
            "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJQmR7Jxnow"
      },
      "source": [
        "When we already have a tensor and want to add names (but not change existing ones), we can call the `refine_names` method  on it. Similar to indexing, the ellipsis `...` allows you to leave out refine_names … any number of dimensions. With the  `rename` sibling method you can also overwrite or drop (by passing in `None`) existing names.\n",
        "\n",
        "当我们已经有一个张量并想要添加名称（但不改变现有的名称）时，我们可以在它上面调用`refine_names`方法。类似于索引，省略号`...`允许你省略任意数量的维度不用在`refine_names`中指定。通过使用`rename`这个兄弟方法，你也可以覆盖或删除（通过传入`None`）现有的名称。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kunAYEsnxnox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770d2222-60f0-4128-eb6b-b00107348c8e"
      },
      "source": [
        "img_named =  img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "#这行代码对张量img_t使用了refine_names方法，其中...用来表示出了最后三个维度外的其他任意数量的维度，不需要在refine_names中指定\n",
        "#后面的三个名称是位张量的倒数后三个维度设定新的名称，如果这些维度之前已经有名称，则将会覆盖原有的名称\n",
        "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "#为batch_t张量添加新的维度名称，...在这里用来表示第一个维度批次带下哦，然后为后面三个维度添加上新的维度名称\n",
        "\n",
        "#分别打印出两个张量的形状和维度名称\n",
        "print(\"img named:\", img_named.shape, img_named.names)\n",
        "print(\"batch named:\", batch_named.shape, batch_named.names)#其中因为并未给彼此大小指定维度名称，所以这个维度的名称显示为None\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
            "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#通过使用rename这个方法也可以覆盖或者删除（通过传入None）现有的名称\n",
        "\n",
        "# 假设有一个具有命名维度的张量\n",
        "batch_named = torch.randn(2, 3, 5, 5, names=('batch', 'channels', 'rows', 'columns'))\n",
        "\n",
        "# 使用rename方法覆盖和删除名称\n",
        "# 覆盖'channels'为'color_channels'，删除'rows'和'columns'的名称\n",
        "renamed_batch = batch_named.rename(batch='batch', channels='color_channels', rows=None, columns=None)\n",
        "#删除了channels的名字，以及更改了rows和columns这两个维度的名称\n",
        "renamed_img=img_named.rename(channels=None,rows='img_rows',columns='img_columns')\n",
        "\n",
        "# 输出原始张量和修改后的张量的维度名称\n",
        "print(img_named.names,renamed_img.names)\n",
        "print(batch_named.names, renamed_batch.names)\n"
      ],
      "metadata": {
        "id": "8XardLJtWNMD",
        "outputId": "3640c140-cb76-41b6-ba9a-c8c59ec40662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('channels', 'rows', 'columns') (None, 'img_rows', 'img_columns')\n",
            "('batch', 'channels', 'rows', 'columns') ('batch', 'color_channels', None, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQhphHyxnoy"
      },
      "source": [
        "For operations with two inputs, in addition to the usual dimension checks, i.e. that sizes are either the same or one is 1 and can be broadcast to the other, PyTorch will now check the names for us. So far, it does not automatically align dimensions, so we need to do this explicitly. The method  `align_as` returns a tensor with missing dimensions added and existing ones permuted to the right order\n",
        "\n",
        "对于具有两个输入的操作，除了通常的维度检查之外，即大小要么相同，要么一个为1并且可以广播到另一个上，PyTorch现在还会为我们检查维度的名称。到目前为止，它不会自动对齐维度，所以我们需要显式地进行这一操作。`align_as`方法会返回一个张量，其中缺失的维度被添加，并且现有的维度被排列到正确的顺序。（`align_as`返回一个新的张量，这个张量会被添加和重排，以匹配目标张量的维度顺序）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwQCsnCFxnoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e14ecf8-92da-459e-ed70-cb3e889eb8d4"
      },
      "source": [
        "weights_aligned = weights_named.align_as(img_named)\n",
        "#通过调用align_as()方法获得一个新的张量，它的维度被重新排列并且依据img_named添加了缺失的维度，从而与img_named的维度顺序相匹配\n",
        "weights_aligned.shape, weights_aligned.names\n",
        "#最后得到新生成的张量的维度形状和名称都与img_named相同"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T10CfSK7xnoz"
      },
      "source": [
        "Functions accepting dimension arguments, like `sum`, also take named dimensions. A nice feature for robustness is that if you try to combine dimensions with different names, you get an error.  Named tensors have the potential of eliminating many sources of alignment errors which are a frequent source of debugging problems.\n",
        "\n",
        "接受维度参数的函数，如sum，也接受命名维度。一个增加鲁棒性的好特性是，如果你尝试合并具有不同名称的维度，你会得到一个错误。命名张量有潜力消除许多由于维度对齐错误引起的问题，这些错误是调试问题的常见来源。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xwtYZuxnoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8041c5-d9a5-40e2-9f40-10cc7d3098f2"
      },
      "source": [
        "#将img_named和weights_aligned进行元素乘法，并对channels维度进行求和，来计算加权灰度图像gray_named\n",
        "gray_named = (img_named * weights_aligned).sum('channels')\n",
        "#两个张量先进行元素乘法，weights_aligned已经与img_named的维度对齐，保证了算法按照正确的维度进行\n",
        "#结果是每个像素的RGB值被相应的权重加权，为转换为灰度图作准备\n",
        "#.sum()是对结果张量沿着颜色通道维度求和，以计算出加权灰度值，将每个像素的加权RGB值合并成单一的灰度值\n",
        "#最后求的每个像素点的加权灰度值\n",
        "gray_named.shape, gray_named.names\n",
        "#最后原始的channels维度通过求和操作被消除，所以最后是2D张量"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), ('rows', 'columns'))"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25l5sirYx-zN"
      },
      "source": [
        "The code below should create an error...\n",
        "\n",
        "下面的代码应该会产生一个错误..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrGFk3txno1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e1685ed7-a1a6-4208-9be1-2387c8c502df"
      },
      "source": [
        "gray_named = (img_named[..., :3] * weights_named).sum('channels')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-4959ba21081c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_named\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_named\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_named\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsY17nfNxno3"
      },
      "source": [
        "***Tensor Element Types***\n",
        "Python numeric types can be sub-optimal for several reasons:\n",
        "* Numbers in Python are full-fledged objects. while a floating point number might only take, for instance, 32 bits to be represented on a computer, Python will convert them in a full-fledged Python object with reference counting, etc.. This operation, called boxing, is not a problem if we need to store a small number of them, but allocating millions of such numbers gets very inefficient;\n",
        "* Lists in Python are meant for sequential collections of objects. there are no operations defined for, say, efficiently taking the dot product of two vectors, or summing vectors together; also, Python lists have no way of optimizing the layout of their content in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers); last, Python lists are one-dimensional, and while one can create lists of lists, this is again very inefficient;\n",
        "* The Python interpreter is slow compared to optimized, compiled code. Performing mathematical operations on large collections of numerical data can be much faster using optimized code written in a compiled, low-level language like C.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRXiM_n8xno4"
      },
      "source": [
        "For these reasons, data science libraries rely on NumPy, or introduce dedicated data structures like PyTorch tensors, that provide efficient low-level implementations of numerical data structures and related operations on them, wrapped in a convenient high-level API. To enable this, the objects within a tensor must be all numbers of the same type and PyTorch must keep track of this numeric type.\n",
        "\n",
        "The `dtype` argument to tensor constructors (that is, functions like `tensor`, `zeros`, `ones`) specifies the numerical data (d) type that will be contained in the tensor. The data type specifies the possible values the tensor can hold (integers vs. floating point numbers) and the number of bytes per value.  The `dtype` argument is deliberately similar to the standard NumPy argument of the same name.\n",
        "\n",
        "Computations happening in neural networks are typically executed in 32-bit floating point precision. Higher precision, like 64-bit, will not buy us improvements in the accuracy of a model and will require more memory and computing time. The 16-bit floating point, half precision data type is not present natively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to half-precision to decrease the footprint of a neural network model if needed, with minor impact on accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGZsoE6ixno5"
      },
      "source": [
        "PyTorch tensors can be converted to NumPy arrays and vice versa very efficiently. By doing so, we can leverage the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This zero-copy interoperability with NumPy arrays is due to the storage system working with the Python buffer protocol Converting tensors to `numpy` arrays is very straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEMha9-Hxno5"
      },
      "source": [
        "points = torch.ones(3, 4)\n",
        "points_np = points.numpy()\n",
        "points_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg-qvoNxno6"
      },
      "source": [
        "which will return a NumPy multidimensional array of the right size, shape and numerical type. Interestingly, the returned array shares the same underlying buffer with the tensor storage. This means that the `numpy` method can be effectively executed at basically no cost, as long as the data  sits in CPU RAM. It also means that modifying the NumPy array will lead to a change in the originating tensor.\n",
        "If the tensor is allocated on the GPU, PyTorch will make a copy of the content of the tensor into a NumPy array allocated on the CPU.\n",
        "Vice-versa, we can obtain a PyTorch tensor from a NumPy array this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DJDbWH8xno7"
      },
      "source": [
        "points = torch.from_numpy(points_np)\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skkE5HKfxno8"
      },
      "source": [
        "While the default numeric type in PyTorch is 32 bit floating point, for the one for numpy it is 64 bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymCxDxLhxno8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5)\n",
        "print(a, a.dtype)\n",
        "b = a.numpy()\n",
        "print(b, b.dtype)\n",
        "\n",
        "c = np.ones(5)\n",
        "print(c, c.dtype)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn3ZFASKxno9"
      },
      "source": [
        "Note how the `numpy` array changes value if we change the `tensor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqGJ69GRxno9"
      },
      "source": [
        "print(a)\n",
        "print(b)\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyc45I7mxnpB"
      },
      "source": [
        "***Serialising tensors***\n",
        "\n",
        "Creating a tensor on the fly is all well and fine, but if the data inside is of any value to us, we will want to save it to a file and load it back at some point. After all, we don’t want to have to retrain a model from scratch every time we start running our program! PyTorch uses `pickle` under the hood to serialize the tensor object, plus dedicated serialization code for the storage. Here’s how we can save our `points` tensor to an `ourpoints.t` file: (note that this won't work as is in `colab` but would be fine if you ran the notebook locally on your machine -- we'll cover colab file issues later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow1F91yRxnpB"
      },
      "source": [
        "torch.save(points, './drive/../data/p1ch3/ourpoints.t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99E4BkSYC5T"
      },
      "source": [
        "One way to save and load data from and to colab is to mount a google drive (you will get some space with your account if you created it for colab)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjmqXdoBX30f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFk5xlNcxnpC"
      },
      "source": [
        "As an alternative, we can pass a file descriptor in lieu of the filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-4a6L_OYXeo"
      },
      "source": [
        "torch.save(points, '/content/drive/MyDrive/ourpoints.t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHG5EnYqxnpC"
      },
      "source": [
        "with open('/content/drive/MyDrive/ourpoints.t','wb') as f:\n",
        "   torch.save(points, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpKDg6uxnpD"
      },
      "source": [
        "points = torch.load('/content/drive/MyDrive/ourpoints.t')\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaXNHWhuxnpF"
      },
      "source": [
        "with open('/content/drive/MyDrive/ourpoints.t','rb') as f:\n",
        "   points = torch.load(f)\n",
        "\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ocH-FqoxnpI"
      },
      "source": [
        "While this is a way we can quickly save tensors in case we only want to load them with PyTorch, the file format itself is not interoperable. We can’t read the tensor with software other than PyTorch. Depending on the use case, this may or may not be a limitation, but we should learn how to save tensors interoperably for those times it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfiLH5WxnpI"
      },
      "source": [
        "***Serialising to HDF5 with h5py***\n",
        "\n",
        "For those cases when you need to, however, you can use the HDF5 format and library. HDF5 is a portable and widely supported format for representing serialized multidimensional arrays, organized in a nested key-value dictionary. Python supports HDF5 through the  library `h5py`, which accepts and returns data under the form of NumPy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIonJBmBxnpJ"
      },
      "source": [
        "import h5py\n",
        "\n",
        "f = h5py.File('/content/drive/MyDrive/ourpoints.hdf5', 'w')\n",
        "dset = f.create_dataset('coords', data=points.numpy())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOBe_T1oxnpJ"
      },
      "source": [
        "Here `coords` is a key into the HDF5 file. We can have other keys, even nested ones. One of the interesting things in HDF5 is that we can index the dataset while on disk and access only the elements we are interested in. Let us suppose we want to load just the last two points in our dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjpHSQYlxnpK"
      },
      "source": [
        "f = h5py.File('/content/drive/MyDrive/ourpoints.hdf5', 'r')\n",
        "dset = f['coords']\n",
        "last_points = dset[-2:]\n",
        "last_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew8KQl8rxnpL"
      },
      "source": [
        "What happened here is that data has not been loaded when the file was opened or the dataset was required. Rather, data stayed on disk until we requested the second and last rows in the dataset. At that point `h5py`,  has accessed those two columns and returned a NumPy array-like object encapsulating that region in that dataset that behaves like a NumPy array and has the same API.\n",
        "Owing to this fact, we can pass the returned object to the `torch.from_numpy` function to obtain  a tensor directly. Note that in this case the data is copied over to the tensor’s storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_5gVEZHxnpL"
      },
      "source": [
        "last_points = torch.from_numpy(dset[-2:])\n",
        "f.close()\n",
        "last_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRiuldkaxnpP"
      },
      "source": [
        "***Moving tensors to the GPU***\n",
        " Every Torch tensor can be transferred to (one of) the GPU(s) in order to perform massively parallel, fast computations. All operations that will be performed on the tensor will be carried out using GPU-specific routines that come with PyTorch. In addition to the `dtype`, a PyTorch `Tensor` also has a notion of `device`, which is where on the computer the tensor data is being placed. Here is how we can create a tensor on the GPU by specifying the corresponding argument to the constructor:\n",
        "(Note: if running this in Colab you must have changed 'runtime' to 'GPU' for this to work - in the Edit/Notebook Settings menu item.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqoZM8f4xnpQ"
      },
      "source": [
        "import torch # Doing this again since your Notebook would have been reset if you changed settings.\n",
        "\n",
        "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
        "points_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_8pRs7xnpR"
      },
      "source": [
        "We could instead copy a tensor created on the CPU onto the GPU using the  method `to()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6UFWdcYxnpR"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cpu')\n",
        "points_gpu = points.to(device='cuda')\n",
        "\n",
        "points_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EOQV9eYxnpT"
      },
      "source": [
        "Doing so returns a new tensor that has the same numerical data, but stored in the RAM of the GPU, rather than in regular system RAM. Now that the data is stored locally on the GPU, we’ll start to see the speedups mentioned earlier when performing mathematical operations on the tensor. In almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making it much easier to write code that is agnostic to where, exactly, the heavy number crunching is running.\n",
        "\n",
        "In case our machine has more than one GPU, we can also decide on which GPU we allocate the tensor by passing a zero-based integer identifying the GPU on the machine, such as\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r50REDPlxnpU"
      },
      "source": [
        "points_gpu = points.to(device='cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQq-KkCGxnpV"
      },
      "source": [
        "points = 2 * points                         # <1>\n",
        "points_gpu = 2 * points.to(device='cuda')   # <2>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6SjS9ExnpV"
      },
      "source": [
        "1. Multiplication performed on the CPU.\n",
        "2. Multiplication performed on the GPU.\n",
        "\n",
        "Note that the  tensor is not brought back to the CPU once the result has been points_gpu computed. What happened in the line above is that\n",
        "\n",
        "1) the  tensor has been copied to the GPU;\n",
        "2) a new tensor has been allocated on the GPU points and used to store the result of the multiplication;\n",
        "3) a handle to that GPU tensor is returned.\n",
        "\n",
        "Therefore, if we also add a constant to the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FOJWiGixnpW"
      },
      "source": [
        "points_gpu = points_gpu + 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8hV1XA-xnpX"
      },
      "source": [
        "the addition is still performed on the GPU, no information flows to the CPU (except if we print or access the resulting tensor). In order to move the tensor back to the CPU we need to provide a `cpu` argument to the `to` method, such as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Prn7LkxnpX"
      },
      "source": [
        "points_cpu = points_gpu.to(device='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCq26o3-Ltz5"
      },
      "source": [
        "There are also specific methods to transfer tensors between the CPU and GPU(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnuxJC2axnpY"
      },
      "source": [
        "points_gpu = points.cuda()\n",
        "points_gpu = points.cuda(0)\n",
        "points_cpu = points_gpu.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FjlpBUexnpZ"
      },
      "source": [
        "***The tensor API***\n",
        "\n",
        "Let's get a feel for the tensor operations that PyTorch offers, to give a feel for the API. The vast majority of operations on and between tensors are available under the `torch` module https://pytorch.org/docs/stable/torch.html and can also be called as methods of a tensor object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5-klzrwxnpZ"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = torch.transpose(a, 0, 1)\n",
        "\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FD1r__xnpa"
      },
      "source": [
        "or, for exactly the same result, as a method of the `a` tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rssecS1Kxnpa"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = a.transpose(0, 1)\n",
        "\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7TSseWDxnpb"
      },
      "source": [
        "For more details look at the online docs http://pytorch.org/docs They are exhaustive and well organized, with the tensor operations divided into groups:\n",
        "* Creation ops — functions for constructing a tensor, like `ones`  and `from_numpy`\n",
        "* Indexing, slicing, joining, mutating ops — functions for changing the shape, stride or content a tensor, like `transpose`\n",
        "* Math ops — functions for manipulating the content of the tensor through computations\n",
        "    * Pointwise ops — functions for obtaining a new tensor by applying a function to each element independently, like `abs` and  `cos`\n",
        "    * Reduction ops — functions for computing aggregate values by iterating through tensors, like `mean`, `std` and `norm`\n",
        "    * Comparison ops — functions for evaluating numerical predicates over tensors, like `equal` and `max`\n",
        "    * Spectral ops — functions for transforming in and operating in the frequency domain, like `stft` and `hamming_window`\n",
        "    * Other operations — special functions operating on vectors, like `cross`, or matrices, like `trace`\n",
        "    * BLAS and LAPACK operations — functions following the BLAS (Basic Linear Algebra Subprograms) specification for scalar, vector-vector, matrix-vector and matrix-matrix operations\n",
        "* Random sampling — functions for generating values by drawing randomly from probability distributions, like `randn` and `normal`\n",
        "* Serialization — functions for saving and loading tensors, like `load` and `save`\n",
        "* Parallelism — functions for controlling the number of threads for parallel CPU execution, like `set_num_threads`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCDrPfaJxnpd"
      },
      "source": [
        "***Summary***\n",
        "\n",
        "* Neural networks transform floating point representations into other floating point representations, with the starting and ending representations typically being human-interpretable. The intermediate representations are less so.\n",
        "* These floating point representations are stored in Tensors.\n",
        "* Tensors are multidimensional arrays; they are the basic data structure in PyTorch.\n",
        "* PyTorch has a comprehensive standard library for tensor creation, manipulation and mathematical operations.\n",
        "* Tensors can be serialized to disk and loaded back.\n",
        "* All tensor operations in PyTorch can execute on the CPU as well as on the GPU, with no change in the code.\n",
        "* PyTorch uses a trailing underscore to indicate that a function operates in-place on a tensor (e.g. `Tensor.sqrt_`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCSKmTNvXy_z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}