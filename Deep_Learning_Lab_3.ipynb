{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "69z7xsgG0dUE"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmf0920/Feiii_code/blob/main/Deep_Learning_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69z7xsgG0dUE"
      },
      "source": [
        "# Lab 3: Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutyeB-fMkS2"
      },
      "source": [
        "#Overall Tasks\n",
        "\n",
        "(See if you can get to Step 3 in the lab itself in case you have questions since the last steps are really calculations and experimenting with the notebook ...)\n",
        "\n",
        "1. It might be good to first read through all this code to get a general idea of the workflow that we will follow.\n",
        "\n",
        "2. Then create the network architecture for a dense MLP (multi-layer perceptron) and a Conv2D network model as given in the Notebook cell below. (You may want to look at the PyTorch documentation for this.)\n",
        "\n",
        "3. Following the Notebook downward - then train each of these models. Run\n",
        "the test data through to see how the models got on. Compare the perfomance of Linear dense model and Conv2D model.\n",
        "\n",
        "4. Can you work out the number of parameters in your dense MLP network and your Conv2D network? Which one is more likely to overfit?\n",
        "\n",
        "5. Use this notebook to experiment with different numbers and sizes of layers and kernel sizes. Note the distinction between the loss (the function we optimise) and accuracy (the actual performance we care about)!\n",
        "\n",
        "6. See how using ADAM to optimise changes learning behaviour compared to SGD.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuYkI8fT0YYH"
      },
      "source": [
        "This week we will get some hands-on experience with convolutional networks on 2D images.\n",
        "\n",
        "After this lab, you should:\n",
        "\n",
        "*   Understand how to build and train a convolutional neural network in PyTorch\n",
        "*   Know how to change the numbers and parameters of layers\n",
        "*   Have a point of comparison for the relative performance of fully connected vs. convolutional networks on a realistic dataset\n",
        "\n",
        "This is quite a challenging lab where **you will have to look up the PyTorch documentation for different classes** (and last week's lab) to write PyTorch models yourself.\n",
        "\n",
        "Again make sure you read through the code and try to work out how it is working. (There will be quite a few unfamiliar things introduced in this lab ... but you will get more used to these as the weeks go on.)\n",
        "\n",
        "First, we will set up a few things:\n",
        "\n",
        "**(IMPORTANT - read through the code! In particular note the comment that says that you should set your runtime on Colab to use a GPU !)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA84zBjcua7y"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize']  = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"pytorch\"\n",
        "\n",
        "# make sure you use the GPU (btw check your runtime is a GPU in colab)\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)\n",
        "    files.download(PROJECT_ROOT_DIR+'/images/'+CHAPTER_ID+'/'+fig_id + \".png\")\n",
        "\n",
        "print(\"Using CUDA ? \" + str(use_cuda))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpzHjuUvxgrr"
      },
      "source": [
        "Now you need to define the network properties.\n",
        "\n",
        "Fill in the missing structure to implement:\n",
        "\n",
        " 1. A multi-layer linear, dense network for `model_dense`\n",
        " 2. A multi-layer convolutional network for `model`.\n",
        "\n",
        "You should know how to build the dense architecture from the last two weeks. Note we suggest the use of the `nn.Sequential` object, the use of an `OrderedDict` structure will allow you to name each layer, for clearer display when you print your model.  \n",
        "\n",
        "For the convnet, remember that the output of every `Conv2d` and `MaxPool2d` layer is a 3D tensor of shape *(channels, height, width)*. The *width* and *height* dimensions tend to shrink as we go deeper in the network (you can use the `padding` parameter to counteract this). The number of input channels is controlled by the first argument passed to the Conv2D layers (e.g. 32 or 64) and the number of kernels / output channels / feature maps is controlled by the second number. Remember to check the impact of padding parameters and maxpooling output dimensions when structuring the dimensions of the conv2d layers.\n",
        "\n",
        "The next step would be to feed our last output tensor (of shape (N, X, Y)) into a densely-connected classifier network like those you are already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D (you can use `nn.Flatten` for this), and then add a few Dense layers after that (be careful to set the correct number of inputs for the dense layers, some calculations by hand are required here!).\n",
        "\n",
        "We are going to do 10-way classification, so use a final layer with 10 outputs and if you want probabilities out, you can use a `nn.Softmax` activation. (Note that if your cost function is [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) you should not use softmax for the output - the CrossEntropyLoss includes it internally!).\n",
        "\n",
        "I would suggest to start with *small* models: it will make the training faster. You can always push a bit the parameters for your last experiments at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQE1avXc0R1Z"
      },
      "source": [
        "import collections\n",
        "\n",
        "model_dense = nn.Sequential(collections.OrderedDict([\n",
        "\n",
        "    # You need to fill this in to create a fully connected network.\n",
        "\n",
        "        ]))\n",
        "\n",
        "model = nn.Sequential(collections.OrderedDict([\n",
        "\n",
        "    # You need to fill this in to create a convolutional network.\n",
        "\n",
        "        ]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eDL77wM2CfU"
      },
      "source": [
        "Now here's what the architecture of our two networks that you have developed looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs6msmo32zV3"
      },
      "source": [
        "print(\"Dense MLP Model:\")\n",
        "print(model_dense)\n",
        "\n",
        "print(\"\\nCNN Network Model:\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3SOqIE02Tur"
      },
      "source": [
        "Now, let's train our ConvNet on the Fashion MNIST digits. You can[ learn more about the Fashion-MNIST data set](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "We specify the root directory to store the dataset, download the training data, if not present on the local machine, and then apply the transforms. ToTensor to turn images into Tensor so we can directly use it with our network. The dataset is stored in the dataset class named `train_set` and the test data in (unsurprisingly) `test_set`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ficLeQynybw1"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Use standard FashionMNIST dataset\n",
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "print(train_set, test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGBQKeHIGYAe"
      },
      "source": [
        "Then we need to define functions for training and testing our networks, the training loop here is fairly standard and is very much the same as in previous weeks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGCMMnNBwk3t"
      },
      "source": [
        "import datetime\n",
        "epoch_print_gap = 1\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, device, loss_fn, train_loader):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            outputs = model(imgs.to(device))\n",
        "            loss = loss_fn(outputs, labels.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % epoch_print_gap == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch, float(loss_train)))\n",
        "\n",
        "def test_loop(model, device, test_loader):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVTyPUdIDNz9"
      },
      "source": [
        "We then initialise the parameters for the training of our networks: optimiser (Stochastic Gradient Descent here, but you can also try Adam), and data loader to pack your training data in mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmHOauL40nNv"
      },
      "source": [
        "lr = 0.01\n",
        "\n",
        "# set up a loader object to use minibatches\n",
        "loader = torch.utils.data.DataLoader(train_set, batch_size = 32)\n",
        "\n",
        "# also set up a loader for test data, using a single batch\n",
        "test_loader = torch.utils.data.DataLoader(test_set)\n",
        "\n",
        "# we set the optimiser as Stochastic Gradient Descent for both networks,\n",
        "# you could also try Adam\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer_dense = optim.SGD(model_dense.parameters(), lr=lr)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#optimizer_dense = optim.Adam(model_dense.parameters(), lr=lr)\n",
        "\n",
        "# show the first batch of training data\n",
        "images, labels = next(iter(loader))\n",
        "grid = torchvision.utils.make_grid(images)\n",
        "plt.imshow(np.transpose(grid.numpy(), (1,2,0)), interpolation='nearest')\n",
        "\n",
        "# set the loss function to optimise. Cross Entropy is usually the best for\n",
        "# classification\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seClxybaFQnF"
      },
      "source": [
        "Finally, we can call the training loop on both models to see how they train. It is a good idea here to ensure some output in your training function so that you get a feeling how fast (and how well) the training goes and that you can abort a training that would be unreasonably long...\n",
        "\n",
        "You could also use `tensorboard` for this purpose, but it is a bit messier to set up, so we will only use printouts at this stage.\n",
        "\n",
        "(https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHM4onO8wr3v"
      },
      "source": [
        "\n",
        "# set the number of epochs: The number of time that the whole training set\n",
        "# will be seen by the network during training\n",
        "n_epochs = 2 # see how sensitive results are to this\n",
        "\n",
        "\n",
        "# train the CNN\n",
        "training_loop(\n",
        "    n_epochs = n_epochs,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    device = device,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = loader,\n",
        ")\n",
        "\n",
        "\n",
        "# train the dense model\n",
        "training_loop(\n",
        "    n_epochs = n_epochs,\n",
        "    optimizer = optimizer_dense,\n",
        "    model = model_dense,\n",
        "    device = device,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = loader,\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then run the test loop using the two different models to assess performance."
      ],
      "metadata": {
        "id": "zxOKQhwdOcaO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa7UGRq5E8To"
      },
      "source": [
        "\n",
        "test_loop(model = model, device = device, test_loader = test_loader)\n",
        "test_loop(model = model_dense, device = device, test_loader = test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well did your two models do for this task ?\n",
        "\n",
        "For \"similar sized\" networks - you should find the CNN network outperforms a fully connected one.\n",
        "\n",
        "Also - when the fully connected one has vastly more parameters (as described in the lecture) - you should find that it performs poorly on the test data due to overfitting.\n",
        "\n",
        "Try experimenting with your network architectures and different approaches to optimization (does SGD or Adam optimize better in this case?) What happens if you set the learning rate (\"lr\") to a very high value? What happens if you set it to a very low value?"
      ],
      "metadata": {
        "id": "_V_S4JkKTgum"
      }
    }
  ]
}