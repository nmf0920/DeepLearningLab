{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmf0920/Feiii_code/blob/main/DL_Lab_7_Time_Series_and_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cQ5BVC_3smx"
      },
      "source": [
        "# Lab 7 - Learning with time: Time Series and Recurrent Neural Networks\n",
        "\n",
        "In this lab you will apply the content from the recurrent networks lecture, about how to predict time series.\n",
        "\n",
        "At the end of this lab, you should know:\n",
        "* How to load time-series data for processing\n",
        "* How to visualise and handle such data in Python\n",
        "* How to build and train a recurrent neural network from such data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LstcuXy5ATLW"
      },
      "source": [
        "from __future__ import division, print_function, unicode_literals,  absolute_import\n",
        "%load_ext autoreload\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KZ4iNw52RkK"
      },
      "source": [
        "#What are time-series?\n",
        "\n",
        "Time series analysis is a subfield of statistics and econometrics. A time-series is a sequence of observations recorded at regular time intervals, so time series data $y_t$ is indexed by time $t$ and ordered sequentially. This presents specific challenges including autocorrelation within the data, non-exchangeability of data points, and non-stationarity of data and parameters. Depending on the frequency of observations, a time series may typically be hourly, daily, weekly, monthly, quarterly and annual. Sometimes, you might have seconds and minute-wise time series as well, like, number of clicks and user visits every minute etc.\n",
        "\n",
        "Because of the sequential nature of the data, time series analysis has particular goals. We can summarize these goals into one of description of a time series in terms of latent components or features of interest, and prediction, which aims to produce reasonable forecasts of the future. Analysis of time-series is the preparatory step before you develop a forecast of the series, and time series forecasting has enormous commercial significance because most information that is important to an organisation, e.g. demand and sales, number of visitors to a website, stock price etc are essentially time series data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEwGR4z2gDQ"
      },
      "source": [
        "## So how do we import time series data?\n",
        "\n",
        "The data for a time series are typically stored in .csv files or other spreadsheet formats and contains two columns: the date/time and the measured value. Letâ€™s use the `read_csv()` in the `pandas` package to read the time series dataset (a csv file on Australian Drug Sales) as a pandas dataframe. Adding the `parse_dates=['date']` argument will make the date column to be parsed as a date field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGNckXuqxpBz"
      },
      "source": [
        "from dateutil.parser import parse\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "plt.rcParams.update({'figure.figsize': (10, 7), 'figure.dpi': 120})\n",
        "\n",
        "# Import as Dataframe\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpgdlzOxySDz"
      },
      "source": [
        "Alternately, you can import it as a pandas `Series` with the date as index. You just need to specify the `index_col` argument in the `pd.read_csv()` to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO7lCZsmyRRt"
      },
      "source": [
        "ser = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
        "ser.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxS9jq3ymgo"
      },
      "source": [
        "## Visualising a time-series\n",
        "We can use `matplotlib` to take a look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGHSZeoysQc"
      },
      "source": [
        "# Time series data source: fpp pacakge in R.\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
        "\n",
        "# Draw Plot\n",
        "def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
        "    plt.figure(figsize=(8,5), dpi=dpi)\n",
        "    plt.plot(x, y, color='tab:red')\n",
        "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
        "    plt.show()\n",
        "\n",
        "plot_df(df, x=df.index, y=df.value, title='Monthly anti-diabetic drug sales in Australia from 1992 to 2008.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--yRNgaxzE1k"
      },
      "source": [
        "## Seasonal plots\n",
        "You might choose to compare years by plotting by month.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr-2fWmczJP3"
      },
      "source": [
        "# Import Data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Prepare data\n",
        "df['year'] = [d.year for d in df.date]\n",
        "df['month'] = [d.strftime('%b') for d in df.date]\n",
        "years = df['year'].unique()\n",
        "\n",
        "# Prep Colors\n",
        "np.random.seed(100)\n",
        "mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False)\n",
        "\n",
        "# Draw Plot\n",
        "plt.figure(figsize=(8,8), dpi= 80)\n",
        "for i, y in enumerate(years):\n",
        "    if i > 0:\n",
        "        plt.plot('month', 'value', data=df.loc[df.year==y, :], color=mycolors[i], label=y)\n",
        "        plt.text(df.loc[df.year==y, :].shape[0]-.9, df.loc[df.year==y, 'value'][-1:].values[0], y, fontsize=12, color=mycolors[i])\n",
        "\n",
        "# Decoration\n",
        "plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='$Drug Sales$', xlabel='$Month$')\n",
        "plt.yticks(fontsize=12, alpha=.7)\n",
        "plt.title(\"Seasonal Plot of Drug Sales Time Series\", fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ4neksgxgvP"
      },
      "source": [
        "Any time series may be split into the following components: Base Level + Trend + Seasonality + Error\n",
        "\n",
        "A trend is observed when there is an increasing or decreasing slope observed in the time series. Whereas seasonality is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors. It could be because of the month of the year, the day of the month, weekdays or even time of the day.\n",
        "\n",
        "However, It is not mandatory that all time series must have a trend and/or seasonality. A time series may not have a distinct trend but have a seasonality. The opposite can also be true.\n",
        "\n",
        "So, a time series may be imagined as a combination of the trend, seasonality and the error terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC1wvOAtxWM-"
      },
      "source": [
        "fig, axes = plt.subplots(1,3, figsize=(9,4), dpi=100)\n",
        "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/guinearice.csv', parse_dates=['date'], index_col='date').plot(title='Trend Only', legend=False, ax=axes[0])\n",
        "\n",
        "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv', parse_dates=['date'], index_col='date').plot(title='Seasonality Only', legend=False, ax=axes[1])\n",
        "\n",
        "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/AirPassengers.csv', parse_dates=['date'], index_col='date').plot(title='Trend and Seasonality', legend=False, ax=axes[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgQ_e2EY15Vn"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJdZlU9xATLg"
      },
      "source": [
        "## Generate data\n",
        "\n",
        "Look at this code and see how it is working.\n",
        "\n",
        "What is the nature of the y_t and y_t2 datapoints?\n",
        "\n",
        "What is the \"gen_func\" function doing?\n",
        "\n",
        "Why would a RNN be appropriate for modelling such data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7FvoFWJfATLh"
      },
      "source": [
        "def gen_func(y_t, y_t2):\n",
        "    e = np.random.normal(scale=0.01)\n",
        "    return (0.8 - 0.5 * np.exp(-y_t**2)) * y_t + 0.1*np.sin(np.pi*y_t) \\\n",
        "        - (0.3 + 0.9 * np.exp(-y_t**2)) * y_t2 + e\n",
        "\n",
        "def generate_data(n=100):\n",
        "    y_t = 0.1\n",
        "    y_t2 = 0.1\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        y_new = gen_func(y_t, y_t2)\n",
        "        data.append([i, y_new, y_t, y_t2])\n",
        "        y_t2 = y_t\n",
        "        y_t = y_new\n",
        "    return np.array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw0sG6qkATLm"
      },
      "source": [
        "data = generate_data(n=1000)\n",
        "\n",
        "# Print out the first 10 data points \"time points\" generated.\n",
        "data[0:9,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw a \"phase diagram\" of the dynamical system\n",
        "\n",
        "What is this diagram showing ?"
      ],
      "metadata": {
        "id": "S5KoSqdJSzCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLDfd-mbATLr"
      },
      "source": [
        "\n",
        "# Plot a phase diagram of the data.\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "ax = plt.subplot(111)\n",
        "plt.xlim((-1.2,1.2))\n",
        "plt.ylim((-1.2,1.2))\n",
        "plt.scatter(data[:,1], data[:,2], alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToEZ-9dpATL2"
      },
      "source": [
        "## Developing a LSTM model of this system.\n",
        "\n",
        "This creates an LSTM model that we will use to model this system.\n",
        "\n",
        "1. Look at the PyTorch documentation for nn.LSTM and try to work out what \"input_dim\", \"latent_dim\" (or \"hidden_dim\"), \"num_layers\" and \"batch_first\" are doing when creating an LSTM layer.\n",
        "\n",
        "2. What is the fully connected linear layer being used for ? What does it get its input from ?\n",
        "\n",
        "3. Look at the PyTorch documentation for nn.utils.rnn.pad_sequence. (Look at the example in the documentation given and make sure you understand this.) How is this being used in the code below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "47wP3kPBATL3"
      },
      "source": [
        "max_length=10\n",
        "\n",
        "def create_model(input_dim, latent_dim, output_dim, num_layers):\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.input_dim  = input_dim\n",
        "            self.hidden_dim = latent_dim\n",
        "            self.num_layers = num_layers\n",
        "            self.rnn1   = nn.LSTM(input_dim, latent_dim, num_layers, batch_first = False)\n",
        "            self.linear = nn.Linear(latent_dim, output_dim)\n",
        "\n",
        "        def init_hidden(self, batch_size):\n",
        "          # This is what we'll initialise our hidden state as\n",
        "          return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
        "                  torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "        def forward(self, x):\n",
        "          batch_size = x.size(1)\n",
        "\n",
        "          h    = self.init_hidden(batch_size)\n",
        "          y, h = self.rnn1(x,h)\n",
        "\n",
        "          y = self.linear(y)\n",
        "\n",
        "          return y, h\n",
        "\n",
        "\n",
        "    model = Net()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model_on_sequences(model, X, y, epochs, verbose=True):\n",
        "\n",
        "    # pad sequences ``pad_sequence`` stacks a list of Tensors along a new dimension,\n",
        "    # and pads them to equal length. For example, if the input is list (batch) of\n",
        "    # sequences with size ``L x *`` and if batch_first is False, returns ``T x B x *``,\n",
        "    # where `T` is length of the longest sequence, `B` is batch size.\n",
        "    X_padded = nn.utils.rnn.pad_sequence(X)\n",
        "    y_padded = nn.utils.rnn.pad_sequence(y)\n",
        "\n",
        "\n",
        "    #####################\n",
        "    # Train model\n",
        "    #####################\n",
        "\n",
        "    hist = np.zeros(epochs)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "    #optimiser = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    for t in range(epochs):\n",
        "        # Clear stored gradient\n",
        "        model.zero_grad()\n",
        "\n",
        "        y_pred, h = model(X_padded)\n",
        "\n",
        "        loss = criterion(y_pred, y_padded)\n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        # Zero out gradient, else they will accumulate between epochs\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimiser.step()\n",
        "    print('Done.')\n",
        "\n",
        "\n",
        "\n",
        "def predict_sequence(model, initial_frames, num_steps):\n",
        "\n",
        "        model.eval()\n",
        "        n, m = initial_frames.shape\n",
        "\n",
        "        preds = [frame for frame in initial_frames]\n",
        "        last_frames = initial_frames\n",
        "\n",
        "        # In each prediction step, we feed in the *whole* (predicted) sequence so far,\n",
        "        # which fits to the stateful=False setup of our LSTM layer (see above).\n",
        "        with torch.no_grad():\n",
        "         for step in range(num_steps):\n",
        "\n",
        "              # current number of frames (incl past predictions):\n",
        "              n_frames = len(last_frames)\n",
        "              d = last_frames.reshape(n_frames, m)\n",
        "              d = torch.from_numpy(d).float()\n",
        "\n",
        "              # pad the input sequence to match the model's expected length:\n",
        "              # Input is a list of tensors.\n",
        "              d_padded = nn.utils.rnn.pad_sequence([d,])\n",
        "\n",
        "              # predict and store:\n",
        "              pred,h = model(d_padded)#,X_lengths)\n",
        "              this_pred = pred[-1,0:1,:].detach().numpy()\n",
        "              preds.append(this_pred)\n",
        "\n",
        "              # add the new prediction to the history/sequence:\n",
        "              last_frames = np.vstack((last_frames, this_pred))\n",
        "\n",
        "        preds = np.array(preds)\n",
        "        model.hidden = model.init_hidden(1)\n",
        "        return preds.reshape(preds.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guG-OEQmATL8"
      },
      "source": [
        "## Helper functions for generating random subsequences for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJh3LsRHATL-"
      },
      "source": [
        "def get_random_subsequence(data, device, seq_length):\n",
        "    n, m = data.shape\n",
        "    index = np.random.randint(0, n-seq_length)\n",
        "    seq = torch.from_numpy(data[index:index+seq_length].reshape(seq_length, -1)).float().to(device)\n",
        "    return seq\n",
        "\n",
        "def generate_random_training_sequence(data, device, seq_length):\n",
        "    frames = get_random_subsequence(data, device, seq_length)\n",
        "    input_frames = frames[:-1]\n",
        "    next_frames = frames[1:]\n",
        "    X = input_frames\n",
        "    y = next_frames\n",
        "    return X, y\n",
        "\n",
        "def generate_random_training_sequences(data, device, n, seq_length):\n",
        "    X_all = []\n",
        "    y_all = []\n",
        "    for i in range(n):\n",
        "        X, y = generate_random_training_sequence(data, device, seq_length)\n",
        "        X_all.append(X)\n",
        "        y_all.append(y)\n",
        "    return X_all, y_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqS1dLNHATMD"
      },
      "source": [
        "## Create the model\n",
        "You can experiment with the impact of changing `latent_dim`.\n",
        "\n",
        "What is this representing in terms of the LSTM structure given in the lecture? (Does this make sense in terms of the number of features input to the linear layer?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfSEKQijATME"
      },
      "source": [
        "# Note: num_layers is the number of recurrent layers.\n",
        "# E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN,\n",
        "# with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
        "\n",
        "model = create_model(input_dim=1, latent_dim=4, output_dim=1, num_layers=1)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_diq-AbzATMI"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Do you understand what is being input to \"generate_random_training_sequences\" ? What is being generated by this function ?\n",
        "\n",
        "For longer training, re-run this cell multiple times, or increase the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWVmbmx5ATMK"
      },
      "source": [
        "X_list, y_list = generate_random_training_sequences(\n",
        "    data[:,1:2], device, n=100, seq_length=10)\n",
        "\n",
        "train_model_on_sequences(model, X_list, y_list, epochs=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCifcnOATMP"
      },
      "source": [
        "### Predict and plot as time series\n",
        "\n",
        "Try changing the parameters `window` and `frame_start`\n",
        "\n",
        "Why do you think it is more accurate if you increase the window size?\n",
        "\n",
        "Why do you think it is more accurate if you predict from a later frame_start ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sRQckMjATMS"
      },
      "source": [
        "frames = data[:,1].reshape(-1,1)\n",
        "\n",
        "frame_start = 1\n",
        "num_preds = 40\n",
        "window = 3\n",
        "\n",
        "preds = predict_sequence(\n",
        "    model,\n",
        "    initial_frames=frames[frame_start:frame_start+window],\n",
        "    num_steps=num_preds)\n",
        "\n",
        "preds = preds.T[0]\n",
        "\n",
        "n=50\n",
        "plt.figure(figsize=(8,4))\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(data[:n,0], data[:n,1], c='k')\n",
        "plt.plot(data[frame_start:frame_start+num_preds+window,0], preds, c='r')\n",
        "plt.scatter(data[frame_start,0], preds[0], s=96, c='r')\n",
        "plt.scatter(data[frame_start+num_preds,0], preds[num_preds], s=96, c='r')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOKQTO4YATMb"
      },
      "source": [
        "Black is the actual sequence, red is the predicted one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2G9BlToATMd"
      },
      "source": [
        "### Predict and plot as phase portrait\n",
        "\n",
        "Again change the window size, num_preds and frame_start to get an idea of how the model is behaving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8PUR0z5ATMe"
      },
      "source": [
        "# Predict model values.\n",
        "\n",
        "frames = data[:,1].reshape(-1,1)\n",
        "\n",
        "frame_start = 1\n",
        "num_preds = 300\n",
        "window = 3\n",
        "\n",
        "preds = predict_sequence(\n",
        "    model,\n",
        "    initial_frames=frames[frame_start:frame_start+window],\n",
        "    num_steps=num_preds)\n",
        "\n",
        "preds = preds.T[0]\n",
        "\n",
        "# Plot phase diagrams.\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "ax = plt.subplot(111)\n",
        "plt.xlim((-1.2,1.2))\n",
        "plt.ylim((-1.2,1.2))\n",
        "plt.scatter(data[:,2], data[:,3], alpha=0.5)\n",
        "plt.scatter(preds[1:], preds[:-1], alpha=0.5, c='r')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdYybdx7ESPM"
      },
      "source": [
        "## Other time-series data\n",
        "Now that you have your model running on one problem, can you adapt it to another? Take e.g. the sunspots or the anti-diabetic drug historical data from above and build an LSTM model. Compare it to trying to predict with static network, based on delayed inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3EwZ_LQAtHe"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(9,4), dpi=100)\n",
        "sunspots_frame = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv', parse_dates=['date'], index_col='date')\n",
        "diabetic_frame = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
        "sunspots_frame.plot(title='Sunspots', legend=False, ax=axes)\n",
        "sunspots_index = sunspots_frame.index\n",
        "sunspots = sunspots_frame.to_numpy()\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(9,4), dpi=100)\n",
        "diabetic_frame.plot(title='Monthly anti-diabetic drug sales in Australia from 1992 to 2008', legend=False, ax=axes)\n",
        "diabetic_index = diabetic_frame.index\n",
        "diabetic = diabetic_frame.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp40tqQoN7KS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}