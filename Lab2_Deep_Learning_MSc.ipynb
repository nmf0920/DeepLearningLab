{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Bp_MPOndKhtI",
        "zOP6ju4SQe40",
        "0Uk5v2UUs1nt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmf0920/Feiii_code/blob/main/Lab2_Deep_Learning_MSc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mFJAviOzTFy"
      },
      "source": [
        "# Lab 2: Experiments with basic MLP networks  and visualising the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read through all the cells that you are running to ensure you mostly understand what the code is doing ! ðŸ™‚\n",
        "## (Not just run cells without looking at them!)\n",
        "## (Modify things ... suggestions in the code ... look up documentation ... print out values ... to understand the code.)\n",
        "\n",
        "Run the following cell to:\n",
        "\n",
        "*   import relevant packages\n",
        "*   setup appropriate matplotlib defaults\n",
        "\n",
        "Make sure you read through this and understand what this is doing.\n"
      ],
      "metadata": {
        "id": "YUUJQkC4ybuB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_jTrbR2snhH"
      },
      "source": [
        "#@title\n",
        "from google.colab import files\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize']  = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_MPOndKhtI"
      },
      "source": [
        "# Understanding simple networks\n",
        "\n",
        "Let's try running some simple multi-layer perceptrons and see how they perform some basic regression and classification tasks. The goal here is to give you some insight into how a network represents the function or classifier, and what the impact of changing the parameters is.\n",
        "\n",
        "Please do experiment with these models and try to work out how they work (a lot of it should look similar to what you did in the 60 minute PyTorch Blitz tutorial).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PORXbgogLCu6"
      },
      "source": [
        "# Task 1 - Regression\n",
        "\n",
        "We can start with a one dimensional regression task. A target system is $$ y = -\\sin(10\\pi (x+0.5)\\sin(x^3))+\\epsilon, $$where $\\epsilon \\sim \\cal{N}(\\mu, \\sigma)$, and $\\mu=0, \\sigma=0.1$.\n",
        "\n",
        "\n",
        "\n",
        "## Examine the following code.\n",
        "\n",
        "Make sure you understand how this code is working (line by line).\n",
        "\n",
        "What are `xgrid`, `ytrue` being used for in this code?\n",
        "\n",
        "What are `x`, `obserr` and `y` in the code?\n",
        "\n",
        "What might you expect `xtest` and `ytest` are used for?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiFzCRkNOiI"
      },
      "source": [
        "import math\n",
        "\n",
        "def testfunc(x):\n",
        "    # You might want to try this alternative function to experiment.\n",
        "    #y = np.sin((4*math.pi*x**2))\n",
        "    y = -np.sin((10*math.pi*(x+0.5)*np.sin((x)**3)))\n",
        "    return y\n",
        "\n",
        "xgrid = torch.t(torch.linspace(-0.5,0.5,100)).unsqueeze(1)\n",
        "ytrue = testfunc(xgrid) # clean data from underlying function (not available for learning)\n",
        "\n",
        "# now create some training data\n",
        "N = 100         # number of noisy training observations made\n",
        "noiseSigma = 0.1   # standard deviation of noise\n",
        "noiseMean = 0.0    # mean of noise\n",
        "\n",
        "x = torch.rand_like(torch.ones(N,1))-0.5\n",
        "obserr = torch.randn(N,1)*noiseSigma + noiseMean\n",
        "y = testfunc(x)+obserr\n",
        "\n",
        "xtest  = torch.rand_like(torch.ones(N,1))-0.5\n",
        "obserr = torch.randn(N,1)*noiseSigma + noiseMean\n",
        "ytest  = testfunc(xtest)+obserr\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(xgrid, ytrue,'-k')\n",
        "plt.plot(x,y,'o')\n",
        "plt.title('True function and observed data')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a simple \"fully connected\" multi-layer perceptron (MLP) network to model this data.\n",
        "\n",
        "Make sure you understand how this code is working - ask questions !\n",
        "\n",
        "How many layers does this network have? How many neurons in the layers? Can you extend the number of layers? How many connections between the different layers? (See the output from the code ...)\n",
        "\n",
        "Do you get an improvement of the loss over time? (Epochs of training)\n"
      ],
      "metadata": {
        "id": "b1D1cIUm8HBC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EptbEK9YrNwo"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "Nhidden = 10    # Try increasing the number of units\n",
        "lr = 0.01\n",
        "epochs = 30000\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # An affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(1, Nhidden)  # See https://pytorch.org/docs/stable/nn.html#linear for documentation\n",
        "\n",
        "        # See later in the notebook why you might want to do this !\n",
        "        # self.fc1a = nn.Linear(Nhidden, Nhidden)\n",
        "        self.fc2 = nn.Linear(Nhidden, 1)  # Can you find out how the weights are initialised? Look in the PyTorch source code https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # try changing this to tanh() or sigmoid()\n",
        "\n",
        "        # See later in the notebook why you might want to do this !\n",
        "        # x = F.tanh(self.fc1a(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(x)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if epoch % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, 0 + 1, running_loss))\n",
        "        running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the model to do predictions!\n",
        "\n",
        "Does it work as well as you would hope?\n",
        "\n",
        "How about if you add in the extra layer into the network? (Need to go back and update the model.) How has it changed the \"architecture\" of the network in terms of layers, number of neurons in those layers and number of connections between the layers? How has this changed the loss over epochs ? How has it changed the fit given below ?"
      ],
      "metadata": {
        "id": "jRumI9myDUFB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R90OoKbIrewK"
      },
      "source": [
        "pred_y = net(x)\n",
        "pred_ygrid = net(xgrid)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xgrid, ytrue,'k-')\n",
        "plt.plot(xgrid, pred_ygrid.detach().numpy(),'b-')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.ylim([-1, 1])\n",
        "plt.title('Regression comparison')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSFcECjUgqQ5"
      },
      "source": [
        "You can view the parameter values of the layers as a list of numpy arrays using the .weight command in pyTorch. Each layer can be accessed independently.\n",
        "\n",
        "Do the number of weights and bias terms printed make sense here given the structure of this network ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBmlCO_uuda5"
      },
      "source": [
        "print('First hidden layer weights', torch.t(net.fc1.weight)) # first hidden layer weights (transposed for ease of inspection)\n",
        "print('First hidden layer biases', net.fc1.bias) # first hidden layer biases\n",
        "print('Second layer weights',net.fc2.weight) # second layer\n",
        "print('Second layer bias', net.fc2.bias) # second layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOP6ju4SQe40"
      },
      "source": [
        "# Do further experimentation on this regression problem to improve it ...\n",
        "\n",
        "Experiment (possibly changing one thing at a time for comparison) with:\n",
        "\n",
        "1. The amount of training data (try 100, 200, 1000 points), and noise levels on the data (try 0.01 and 0.1 for noiseSigma).\n",
        "2. The number of units (neurons) in the model (essentially the \"width\" of the network).\n",
        "3. The choice of activation function.\n",
        "4. Add an extra layer of hidden units to the network and compare performance between two layers of N units and one layer of 2N units (essentially the \"depth\" of the network).\n",
        "\n",
        "Compare these by plotting a visualisation of the network output at `xgrid` inputs, and compare it with the `ytrue` results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MmfdNtxRQv0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Task 2 - Classification\n",
        "\n",
        "Now we will try out some experiments with simple two-dimensional classification tasks. Again read through the code to make sure you feel you understand how it is working. For instance, how does meshgrid() work? Look it up in the PyTorch documentation if you are not sure what it is doing. See if you can work out the shape (or size) of the tensors involved (can always print out value.shape to checkthis !) For instance, what shape does `x` have in this code ?\n",
        "\n",
        "\n",
        "**Dataset 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-AApql6Rc7T"
      },
      "source": [
        "N = 200\n",
        "\n",
        "xgrid = torch.arange(0,1.0,0.01)\n",
        "\n",
        "X,Y = torch.meshgrid(xgrid, xgrid)\n",
        "testgrid = torch.zeros((np.shape(xgrid)[0]**2,2))\n",
        "testgrid[:,0] = X.flatten()\n",
        "testgrid[:,1] = Y.flatten()\n",
        "\n",
        "\n",
        "# What distribution is produced by torch.randn ?\n",
        "# Try experimenting with different types of input distribution\n",
        "# x = np.random.uniform(low = 0.0, high = 1.0, size= (N,2))\n",
        "\n",
        "x = 0.2*torch.randn(N,2)+torch.tensor([0.5, 0.5])\n",
        "\n",
        "y = ((torch.linalg.vector_norm(x-0.5, dim=1) < 0.2)*1.0).type(torch.float)  # make the class be a circle in the centre of the scene.\n",
        "\n",
        "# cut out a border around the classes to make the task a bit more realistic\n",
        "ind_incl = (torch.linalg.vector_norm(x-0.5, dim=1) > 0.25) | (torch.linalg.vector_norm(x-0.5, dim=1) < 0.20)\n",
        "x = x[ind_incl,:]\n",
        "y = y[ind_incl]\n",
        "N = y.shape[0]\n",
        "y = y.reshape(N,1)\n",
        "print(N)\n",
        "\n",
        "Ntest = 1000\n",
        "xtest = 0.2*torch.randn(Ntest,2)+torch.tensor([0.5, 0.5])\n",
        "ytest = ((torch.linalg.vector_norm(xtest-0.5, dim=1) < 0.2)*1.0).type(torch.float)\n",
        "\n",
        "# cut out a border around the classes to make the task a bit more realistic\n",
        "ind_incl = (torch.norm(xtest-0.5, dim=1) > 0.25) | (torch.norm(xtest-0.5, dim=1) < 0.20)\n",
        "xtest = xtest[ind_incl,:]\n",
        "ytest = ytest[ind_incl]\n",
        "\n",
        "Ntest = ytest.shape[0]\n",
        "ytest = ytest.reshape(Ntest,1)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x[:,0].numpy(),x[:,1].numpy(), alpha=0.2, s=90, cmap='viridis', c=y.view(-1))\n",
        "plt.title('Dataset 1')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Define a suitable MLP or \"fully-connected\" model\n",
        "\n",
        "This type of simple network can alternatively be created using the nn.Sequential container. See this [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) class page in the PyTorch documentation for an example of this.\n",
        "\n",
        "\n",
        "What is the size of the input to this model? What is the size of output? Why do we have a sigmoid activation function on the output? (See the printed output.)\n",
        "\n"
      ],
      "metadata": {
        "id": "9ETIjfiJX08h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNXcVNuBeSr-"
      },
      "source": [
        "Nhidden = 10\n",
        "lamb=0.0000    # L2 weight decay term\n",
        "lr = 0.01\n",
        "epochs = 20000\n",
        "\n",
        "\n",
        "class Net2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2d, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(2, Nhidden)\n",
        "        self.fc2 = nn.Linear(Nhidden, Nhidden)\n",
        "        self.fc3 = nn.Linear(Nhidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "net2d = Net2d()\n",
        "print(net2d)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the network\n",
        "\n",
        "What type of loss are we using here? Why are we not using MSELoss ?"
      ],
      "metadata": {
        "id": "Ua3dqui3YXTq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYn_LCd8-Q9n"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net2d.parameters(), lr=lr, momentum=0.9, weight_decay=lamb) # Note that this is adding L2 regularisation with factor lamb to every parameter\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net2d(x)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if epoch % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, 0 + 1, running_loss))\n",
        "        running_loss = 0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8lLWswJeA4"
      },
      "source": [
        "# Using the network to do prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G72CA3VOg33R"
      },
      "source": [
        "pred_test = net2d(xtest)\n",
        "plt.figure()\n",
        "plt.scatter(xtest[:,0],xtest[:,1], alpha=0.2, s=90, cmap='viridis', c=pred_test[:,0].detach().numpy())\n",
        "plt.title('Dataset 1')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n",
        "pred_testgrid = net2d(testgrid)\n",
        "plt.figure()\n",
        "plt.scatter(testgrid[:,0],testgrid[:,1], alpha=0.2, s=90, cmap='viridis', c=pred_testgrid[:,0].detach().numpy())\n",
        "plt.title('Dataset 1 fine response')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n",
        "print('Final training set loss ', criterion(outputs, y).item())\n",
        "test_loss = criterion(pred_test, ytest)\n",
        "print('Test loss ', test_loss.item())\n",
        "classification = torch.round(pred_test)\n",
        "percentage = 100*sum(ytest.type(torch.int) == classification.type(torch.int))/Ntest\n",
        "print('Percentage correct ', percentage.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xN5VjtY_fFZ"
      },
      "source": [
        "# Experiment with the following ...\n",
        "\n",
        "Try repeating these experiments with the following variations:\n",
        "- Try with 200, 500, 2000 training points. How does that change the response of the function?\n",
        "- Try changing the number of hidden units. What changes?\n",
        "    - Do you see as many 'edges' on the decision boundary when using ReLUs as units? Why do you think this is?\n",
        "    - Try with 1,2,3 hidden layers.\n",
        "- Try with Sigmoids in the hidden layer rather than ReLUs.\n",
        "    - How does that change the shape of the decision boundary? Why do you think this is?\n",
        "    - How does it change the speed of error reduction?\n",
        "\n",
        "- Try changing the loss function from Binary Cross-Entropy (`BCEloss`) to `MSEloss`. How does that affect performance? How does it affect perfomance in `mse`?\n",
        "- During the above you may find that some configurations take a lot longer to reduce the error than others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B97AY1r5lv-E"
      },
      "source": [
        "The correct choice of loss function is linked to the output type, as shown in the table below:\n",
        "\n",
        "![output/cost function table](https://docs.google.com/uc?export=download&id=1Z2C-NJh35shzvtMlvEtDpyQgGM10bvew)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Task 3 - Classification\n",
        "\n",
        "\n",
        "Let's do a classification based on the boundary that we used for regression !\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset 2**"
      ],
      "metadata": {
        "id": "XwD3xDdOc_kB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcCBKVNRU-k7"
      },
      "source": [
        "N = 100   # Try 100, 500, 1000\n",
        "\n",
        "xgrid1 = torch.arange(0,1.0,0.01)\n",
        "\n",
        "#try experimenting with different types of input distribution\n",
        "data_dist = 1 # 1 - uniform 0.5-0.5, 2 - normally distributed\n",
        "\n",
        "if data_dist == 1:\n",
        "   x = torch.rand(N,2)\n",
        "else:\n",
        "   x = 0.15*torch.randn(N,2)+torch.tensor([0.5, 0.5])\n",
        "ind_pos = (0.5+0.4*testfunc(x[:,0]-0.5)<x[:,1])\n",
        "y = (ind_pos*1.0).type(torch.float)  # lets create a classification line based on old regression function\n",
        "N = y.shape[0]\n",
        "y = y.reshape(N,1)\n",
        "x[ind_pos,1] = x[ind_pos,1]+0.05 # create a gap between classes.\n",
        "\n",
        "\n",
        "if data_dist == 1:\n",
        "  xtest = torch.rand(N,2)\n",
        "else:\n",
        "  xtest = 0.15*torch.randn(N,2)+torch.tensor([0.5, 0.5])\n",
        "ind_post = (0.5+0.4*testfunc(xtest[:,0]-0.5)<xtest[:,1])\n",
        "ytest = (ind_post*1.0).type(torch.float)  # let's create a classification line based on old regression function\n",
        "Ntest = ytest.shape[0]\n",
        "ytest = ytest.reshape(Ntest,1)\n",
        "xtest[ind_post,1] = xtest[ind_post,1]+0.05 # create a gap between classes.\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x[:,0],x[:,1], alpha=0.2, s=90, cmap='viridis', c=y.view(-1))\n",
        "plt.plot(xgrid1, 0.5+0.4*ytrue,'--k')  # show the 'true' classification line for this problem\n",
        "plt.title('Dataset 2')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model\n",
        "\n",
        "This time we will use the Sequential container that was mentioned above to define this model."
      ],
      "metadata": {
        "id": "4fg18tmJdbxh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooUmecIbhSv_"
      },
      "source": [
        "Nhidden = 10 # experiment with this\n",
        "lamb=0.00\n",
        "lr = 0.01\n",
        "epochs = 50000\n",
        "\n",
        "\n",
        "net2d = nn.Sequential(nn.Linear(2,Nhidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(Nhidden, Nhidden),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(Nhidden, 1),\n",
        "                      nn.Sigmoid())\n",
        "# reuse net2d variable.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "\n",
        "Does the loss change during training? What is this telling you?"
      ],
      "metadata": {
        "id": "XOO9tm4WdqdI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_L2dd6Grpyb"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net2d.parameters(), lr=lr, momentum=0.9, weight_decay=lamb) # Note that this is adding L2 regularisation with factor lamb to every parameter\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net2d(x)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if epoch % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, 0 + 1, running_loss))\n",
        "        running_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do prediction on the test set using the model"
      ],
      "metadata": {
        "id": "oX3py9mNd7-B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyR92Ho8pTyt"
      },
      "source": [
        "pred_test = net2d(xtest)\n",
        "plt.figure()\n",
        "plt.scatter(xtest[:,0],xtest[:,1], alpha=0.2, s=90, cmap='viridis', c=pred_test[:,0].detach().numpy())\n",
        "plt.plot(xgrid1, 0.5+0.4*ytrue,'--k')  # show the 'true' classification line for dataset2 problem\n",
        "plt.title('Dataset 2')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n",
        "pred_testgrid = net2d(testgrid)\n",
        "plt.figure()\n",
        "plt.scatter(testgrid[:,0],testgrid[:,1], alpha=0.2, s=90, cmap='viridis', c=pred_testgrid[:,0].detach().numpy())\n",
        "plt.plot(xgrid1, 0.5+0.4*ytrue,'--k')  # show the 'true' classification line for this problem\n",
        "plt.title('Dataset 2 fine response')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n",
        "print('Final training set loss ', criterion(outputs, y).item())\n",
        "test_loss = criterion(pred_test, ytest)\n",
        "print('Test loss ', test_loss.item())\n",
        "classification = torch.round(pred_test)\n",
        "percentage = 100.0*torch.sum(ytest.type(torch.int) == classification.type(torch.int))/Ntest\n",
        "print('Percentage correct ', percentage.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uk5v2UUs1nt"
      },
      "source": [
        "# Try experimenting with the following aspects of this network:\n",
        "\n",
        "Try changing the distribution of the training data (e.g. make it normally distributed) but keeping the same classification boundary. How does that change the resulting classification boundary?\n",
        "\n",
        "Try with N= 100, 500, 1000 training examples with linearly distribute and normally distributed data. What is this telling you?\n",
        "\n",
        "Try different sizes of network to see what differences result.\n",
        "\n",
        "What happens with small training sets and large networks?"
      ]
    }
  ]
}